{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd97873a-7126-455b-9910-9f993b498260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing the dataset...\n",
      "Dataset 'civic_issues_dataset.csv' already exists. Skipping generation.\n",
      "Dataset loaded. Training with 24000 samples, testing with 6000 samples.\n",
      "\n",
      "Vectorizing text descriptions using TF-IDF...\n",
      "Text vectorization complete.\n",
      "\n",
      "--- Starting Model Comparison ---\n",
      "\n",
      "Training Logistic Regression...\n",
      "‚úÖ Logistic Regression trained in 2.15 seconds.\n",
      "   Average Accuracy (across all outputs): 0.7036\n",
      "\n",
      "--- Detailed Classification Report ---\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "multiclass-multioutput is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 113\u001b[39m\n\u001b[32m    110\u001b[39m \u001b[38;5;66;03m# --- Detailed Report ---\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[38;5;66;03m# This report is still valuable for seeing per-class performance\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Detailed Classification Report ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mclassification_report\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m    115\u001b[39m \u001b[38;5;66;03m# Check if this is the best model so far\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m average_accuracy > best_model_accuracy:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    212\u001b[39m         skip_parameter_validation=(\n\u001b[32m    213\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    214\u001b[39m         )\n\u001b[32m    215\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    218\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    219\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    222\u001b[39m     msg = re.sub(\n\u001b[32m    223\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    224\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    225\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    226\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:2671\u001b[39m, in \u001b[36mclassification_report\u001b[39m\u001b[34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[39m\n\u001b[32m   2563\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Build a text report showing the main classification metrics.\u001b[39;00m\n\u001b[32m   2564\u001b[39m \n\u001b[32m   2565\u001b[39m \u001b[33;03mRead more in the :ref:`User Guide <classification_report>`.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2667\u001b[39m \u001b[33;03m<BLANKLINE>\u001b[39;00m\n\u001b[32m   2668\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2670\u001b[39m y_true, y_pred = attach_unique(y_true, y_pred)\n\u001b[32m-> \u001b[39m\u001b[32m2671\u001b[39m y_type, y_true, y_pred = \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2673\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2674\u001b[39m     labels = unique_labels(y_true, y_pred)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:118\u001b[39m, in \u001b[36m_check_targets\u001b[39m\u001b[34m(y_true, y_pred)\u001b[39m\n\u001b[32m    116\u001b[39m \u001b[38;5;66;03m# No metrics support \"multiclass-multioutput\" format\u001b[39;00m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mbinary\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmulticlass\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmultilabel-indicator\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m is not supported\u001b[39m\u001b[33m\"\u001b[39m.format(y_type))\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mbinary\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmulticlass\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    121\u001b[39m     xp, _ = get_namespace(y_true, y_pred)\n",
      "\u001b[31mValueError\u001b[39m: multiclass-multioutput is not supported"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import joblib\n",
    "import time\n",
    "\n",
    "# --- Import the models we want to compare ---\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# --- 1. LOAD AND PREPARE THE DATA ---\n",
    "print(\"Loading and preparing the dataset...\")\n",
    "\n",
    "def generate_dummy_text_data(filename='civic_issues_dataset.csv'):\n",
    "    \"\"\"Generates a dummy CSV file if it doesn't exist.\"\"\"\n",
    "    if pd.io.common.file_exists(filename):\n",
    "        print(f\"Dataset '{filename}' already exists. Skipping generation.\")\n",
    "        return\n",
    "    print(f\"Creating dummy dataset '{filename}'...\")\n",
    "    data = {\n",
    "        'Description': [\n",
    "            \"The traffic signal at main street is broken and causing jams.\",\n",
    "            \"Huge pothole on the highway near the exit ramp.\",\n",
    "            \"Graffiti spray painted on the park walls again.\",\n",
    "            \"Garbage has not been collected from our neighborhood for a week.\",\n",
    "            \"A streetlight on elm street is flickering constantly.\",\n",
    "            \"Missed garbage pickup for the third time this month.\",\n",
    "            \"Dangerous pothole reported on the corner of 5th and oak.\",\n",
    "            \"The park swings are broken and unsafe for children.\",\n",
    "            \"Illegal dumping of trash in the empty lot.\",\n",
    "            \"The downtown traffic lights are out of sync.\"\n",
    "        ] * 10, # Multiply data to get a larger dataset\n",
    "        'Category': ['Traffic', 'Roads', 'Vandalism', 'Sanitation', 'Utilities', 'Sanitation', 'Roads', 'Parks', 'Sanitation', 'Traffic'] * 10,\n",
    "        'Issue': ['Broken Signal', 'Pothole', 'Graffiti', 'Missed Pickup', 'Broken Streetlight', 'Missed Pickup', 'Pothole', 'Broken Equipment', 'Illegal Dumping', 'Broken Signal'] * 10,\n",
    "        'Severity': ['High', 'Medium', 'Low', 'High', 'Medium', 'High', 'High', 'Medium', 'Medium', 'Low'] * 10\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(\"‚úÖ Dummy dataset created.\")\n",
    "\n",
    "# Generate dummy data if the CSV is not present\n",
    "generate_dummy_text_data()\n",
    "df = pd.read_csv('civic_issues_dataset.csv')\n",
    "\n",
    "# Define features (X) and targets (y)\n",
    "X = df['Description']\n",
    "y = df[['Category', 'Issue', 'Severity']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(f\"Dataset loaded. Training with {len(X_train)} samples, testing with {len(X_test)} samples.\")\n",
    "\n",
    "\n",
    "# --- 2. VECTORIZE THE TEXT DATA ---\n",
    "print(\"\\nVectorizing text descriptions using TF-IDF...\")\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "print(\"Text vectorization complete.\")\n",
    "\n",
    "\n",
    "# --- 3. DEFINE AND COMPARE MODELS ---\n",
    "print(\"\\n--- Starting Model Comparison ---\")\n",
    "\n",
    "# Define the base classifiers you want to compare\n",
    "models_to_compare = {\n",
    "    \"Logistic Regression\": LogisticRegression(solver='liblinear', random_state=42),\n",
    "    \"Linear SVM (LinearSVC)\": LinearSVC(random_state=42, dual=True), # dual=True is often better for this scenario\n",
    "    \"Multinomial Naive Bayes\": MultinomialNB()\n",
    "}\n",
    "\n",
    "results = {}\n",
    "best_model_name = None\n",
    "best_model_accuracy = 0.0\n",
    "best_model_object = None\n",
    "\n",
    "for name, model in models_to_compare.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Wrap each base estimator in a MultiOutputClassifier\n",
    "    multi_output_model = MultiOutputClassifier(estimator=model, n_jobs=-1)\n",
    "    multi_output_model.fit(X_train_tfidf, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = multi_output_model.predict(X_test_tfidf)\n",
    "    y_pred_df = pd.DataFrame(y_pred, columns=y_test.columns, index=y_test.index)\n",
    "    \n",
    "    # --- Calculate Average Accuracy ---\n",
    "    # 'multiclass-multioutput' is not supported by a single accuracy_score call.\n",
    "    # We calculate the accuracy for each output column and then average them.\n",
    "    accuracies = []\n",
    "    for column in y_test.columns:\n",
    "        col_accuracy = accuracy_score(y_test[column], y_pred_df[column])\n",
    "        accuracies.append(col_accuracy)\n",
    "    \n",
    "    # The main metric for comparison will be the average accuracy\n",
    "    average_accuracy = sum(accuracies) / len(accuracies)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "    \n",
    "    print(f\"‚úÖ {name} trained in {training_time:.2f} seconds.\")\n",
    "    print(f\"   Average Accuracy (across all outputs): {average_accuracy:.4f}\")\n",
    "    \n",
    "    # Store results\n",
    "    results[name] = average_accuracy\n",
    "    \n",
    "    # --- Detailed Report ---\n",
    "    # This report is still valuable for seeing per-class performance\n",
    "    print(\"\\n--- Detailed Classification Report ---\")\n",
    "    print(classification_report(y_test, y_pred_df, target_names=y_test.columns.tolist(), zero_division=0))\n",
    "\n",
    "    # Check if this is the best model so far\n",
    "    if average_accuracy > best_model_accuracy:\n",
    "        best_model_accuracy = average_accuracy\n",
    "        best_model_name = name\n",
    "        best_model_object = multi_output_model\n",
    "\n",
    "# --- 4. SAVE THE BEST MODEL ---\n",
    "print(\"\\n--- Comparison Summary ---\")\n",
    "for name, acc in results.items():\n",
    "    print(f\"- {name}: Average Accuracy = {acc:.4f}\")\n",
    "\n",
    "print(f\"\\nüèÜ The best performing model is: **{best_model_name}** with an average accuracy of {best_model_accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nSaving the best model and the vectorizer to disk...\")\n",
    "joblib.dump(best_model_object, 'civic_issue_model.pkl')\n",
    "joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')\n",
    "print(\"‚úÖ Best model and vectorizer saved successfully!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bb45af-b4e4-4c3f-bd63-a68568a8c667",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
