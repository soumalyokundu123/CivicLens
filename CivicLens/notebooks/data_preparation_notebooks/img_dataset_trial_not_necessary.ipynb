{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "537a4f0a-40b2-4dbc-a7b3-96276e7cb6cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting icrawler\n",
      "  Downloading icrawler-0.6.10-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\swarnabha\\appdata\\roaming\\python\\python311\\site-packages (from icrawler) (4.13.3)\n",
      "Collecting bs4 (from icrawler)\n",
      "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
      "Requirement already satisfied: lxml in c:\\users\\swarnabha\\appdata\\roaming\\python\\python311\\site-packages (from icrawler) (6.0.1)\n",
      "Requirement already satisfied: pillow in c:\\users\\swarnabha\\appdata\\roaming\\python\\python311\\site-packages (from icrawler) (11.1.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\swarnabha\\appdata\\roaming\\python\\python311\\site-packages (from icrawler) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\swarnabha\\appdata\\roaming\\python\\python311\\site-packages (from icrawler) (2.32.3)\n",
      "Requirement already satisfied: six in c:\\users\\swarnabha\\appdata\\roaming\\python\\python311\\site-packages (from icrawler) (1.17.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\swarnabha\\appdata\\roaming\\python\\python311\\site-packages (from beautifulsoup4->icrawler) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\swarnabha\\appdata\\roaming\\python\\python311\\site-packages (from beautifulsoup4->icrawler) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\swarnabha\\appdata\\roaming\\python\\python311\\site-packages (from requests->icrawler) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\swarnabha\\appdata\\roaming\\python\\python311\\site-packages (from requests->icrawler) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\swarnabha\\appdata\\roaming\\python\\python311\\site-packages (from requests->icrawler) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\swarnabha\\appdata\\roaming\\python\\python311\\site-packages (from requests->icrawler) (2025.1.31)\n",
      "Downloading icrawler-0.6.10-py3-none-any.whl (36 kB)\n",
      "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Installing collected packages: bs4, icrawler\n",
      "Successfully installed bs4-0.0.2 icrawler-0.6.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install icrawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91528b59-556a-4991-b290-b1eae325aeee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-20 00:34:35,866 - INFO - icrawler.crawler - start crawling...\n",
      "2025-09-20 00:34:35,867 - INFO - icrawler.crawler - starting 1 feeder threads...\n",
      "2025-09-20 00:34:35,868 - INFO - icrawler.crawler - starting 1 parser threads...\n",
      "2025-09-20 00:34:35,869 - INFO - icrawler.crawler - starting 1 downloader threads...\n",
      "2025-09-20 00:34:37,030 - INFO - parser - parsing result page https://www.google.com/search?q=pothole&ijn=0&start=0&tbs=&tbm=isch\n",
      "2025-09-20 00:34:37,327 - INFO - downloader - image #1\thttps://cdn.shopify.com/s/files/1/0274/7288/7913/files/MicrosoftTeams-image_32.jpg\n",
      "2025-09-20 00:34:38,795 - INFO - downloader - image #2\thttps://blogs-images.forbes.com/laurenfix/files/2018/04/Pothole-damage.png\n",
      "2025-09-20 00:34:39,126 - ERROR - downloader - Response status code 403, file https://sripath.com/wp-content/uploads/2025/01/iStock-174662203.jpg\n",
      "2025-09-20 00:34:39,326 - ERROR - downloader - Response status code 403, file https://www.morrisbart.com/wp-content/uploads/2022/05/pothole-in-road-with-cone.jpg\n",
      "2025-09-20 00:34:40,718 - INFO - downloader - image #3\thttps://images.contentstack.io/v3/assets/blt4cb7085064c0b32f/blt5e2a51c67e066df0/672121f8e6953827c3a669ec/Nid-de-poule-recours-voiture.jpg\n",
      "2025-09-20 00:34:40,908 - ERROR - downloader - Response status code 403, file https://www.unionmutual.com/wp-content/uploads/2016/07/Potholes-resized-for-blog.jpg\n",
      "2025-09-20 00:34:42,349 - ERROR - downloader - Response status code 403, file https://jacksoncountyor.gov/Images/Departments/Roads/News/Pothole.jpg\n",
      "2025-09-20 00:34:43,581 - INFO - downloader - image #4\thttps://www.wolfpaving.com/hs-fs/hub/98698/file-16089009.jpg\n",
      "2025-09-20 00:34:44,036 - INFO - downloader - image #5\thttps://www.theaa.com/~/media/the-aa/breakdown-cover/advice/content-images/pothole-in-road.jpg\n",
      "2025-09-20 00:34:45,998 - INFO - downloader - image #6\thttps://www.bituchem.com/wp-content/uploads/2022/09/shutterstock_1924989917.jpg\n",
      "2025-09-20 00:34:46,586 - ERROR - downloader - Response status code 404, file https://upload.wikimedia.org/wikipedia/commons/thumb/8/86/Pothole_in_Villeray%2C_Montr%C3%A9al.jpg\n",
      "2025-09-20 00:34:54,075 - INFO - downloader - image #7\thttps://www.alevelgeography.com/wp-content/uploads/2016/01/pothole.jpg\n",
      "2025-09-20 00:34:54,264 - ERROR - downloader - Response status code 403, file https://newsroom.aaa.com/wp-content/uploads/2022/03/Pothole-Twitter.png\n",
      "2025-09-20 00:34:54,635 - INFO - downloader - image #8\thttps://townsquare.media/site/695/files/2022/11/attachment-Untitled-design-98.jpg\n",
      "2025-09-20 00:34:55,424 - INFO - downloader - image #9\thttps://images.ctfassets.net/v7wr16nrr0mz/5TK4Z2goyfZTTDfLX3JpmU/1f10b7e0dc75868ee262b0a543206152/banner-pothole-patch.jpg\n",
      "2025-09-20 00:34:57,599 - INFO - downloader - image #10\thttps://cdn-bpcia.nitrocdn.com/mTLPGxlyxYDtUvKPHzJETjfUafBlUtYn/assets/images/optimized/rev-338516e/www.fltlaw.com/wp-content/uploads/2019/07/1-main.png\n",
      "2025-09-20 00:34:58,135 - INFO - downloader - image #11\thttps://www.shutterstock.com/image-photo/potentially-tyreripping-waterfilled-potholes-on-260nw-2528669605.jpg\n",
      "2025-09-20 00:34:58,518 - ERROR - downloader - Response status code 404, file https://upload.wikimedia.org/wikipedia/commons/thumb/b/b1/Banbury%27s_Bretch_Hill_Pothole%2C_2010.png\n",
      "2025-09-20 00:34:58,853 - ERROR - downloader - Response status code 400, file https://media.istockphoto.com/id/502561495/photo/pot-holed-road.jpg\n",
      "2025-09-20 00:34:59,109 - INFO - downloader - image #12\thttps://blog.ipleaders.in/wp-content/uploads/2021/01/022018_pothole.jpg\n",
      "2025-09-20 00:35:06,963 - INFO - downloader - image #13\thttps://www.niradynamics.com/hubfs/Road-Health---pothole.png\n",
      "2025-09-20 00:35:09,142 - INFO - downloader - image #14\thttps://www.tyresafe.org/wp-content/uploads/2024/01/Pothole-Partnership-to-Combat-Record-Pothole-Damage.jpg\n",
      "2025-09-20 00:35:09,360 - INFO - downloader - image #15\thttps://ichef.bbci.co.uk/ace/standard/976/cpsprodpb/10DED/production/_91410196_potholes_bath_lady.jpg\n",
      "2025-09-20 00:35:10,828 - INFO - downloader - image #16\thttps://sableasphalt.com/wp-content/uploads/2018/02/pothole.jpg\n",
      "2025-09-20 00:35:12,618 - INFO - downloader - image #17\thttps://www.acplm.net/wp-content/uploads/2024/03/Pothole-Filling-vs.-Pothole-Patching.jpg\n",
      "2025-09-20 00:35:13,085 - INFO - downloader - image #18\thttps://www.shutterstock.com/image-photo/closeup-large-pothole-on-damaged-260nw-2499298641.jpg\n",
      "2025-09-20 00:35:15,317 - INFO - downloader - image #19\thttps://cdn11.bigcommerce.com/s-6y4u68maoa/images/stencil/900x450/uploaded_images/blog-images-14-.jpg\n",
      "2025-09-20 00:35:15,590 - ERROR - downloader - Response status code 400, file https://media.istockphoto.com/id/533964313/photo/road-damage-pot-hole.jpg\n",
      "2025-09-20 00:35:15,910 - INFO - downloader - image #20\thttps://www.motorbiscuit.com/wp-content/uploads/2023/04/car-pothole-in-Slough-U.K..jpg\n",
      "2025-09-20 00:35:16,171 - INFO - downloader - image #21\thttps://www.moneylife.in/site/userimage/image/responsive/Pothole15925.jpg\n",
      "2025-09-20 00:35:19,082 - INFO - downloader - image #22\thttps://www.blownawayusa.com/cfsite3/wp-content/uploads/2017/03/potholes-1500x1125-1.jpg\n",
      "2025-09-20 00:35:22,599 - INFO - downloader - image #23\thttps://pub.mdpi-res.com/electronics/electronics-13-03790/article_deploy/html/images/electronics-13-03790-g001.png\n",
      "2025-09-20 00:35:24,576 - INFO - downloader - image #24\thttps://sfpublicworks.org/sites/default/files/2024-10/texas-pothole-635x424.jpg\n",
      "2025-09-20 00:35:24,821 - INFO - downloader - image #25\thttps://www.hindustantimes.com/ht-img/img/2025/09/12/1600x900/Mumbai--India---July-3--2019-Pothole-Repiring-work_1757704736029.jpg\n",
      "2025-09-20 00:35:26,347 - INFO - downloader - image #26\thttp://www.pavementinteractive.org/wp-content/uploads/2007/08/Kailua_pothole.jpg\n",
      "2025-09-20 00:35:28,253 - INFO - downloader - image #27\thttps://www.cyberswift.com/blog/wp-content/uploads/2023/07/AI-Based-Pothole-Detection-1140x641.jpg\n",
      "2025-09-20 00:35:29,859 - INFO - downloader - image #28\thttps://www.hauptman-obrien.net/wp-content/uploads/2014/05/pothole-safety-omaha-personal-injury-attorneys.jpg\n",
      "2025-09-20 00:35:33,069 - INFO - downloader - image #29\thttps://imagevision.ai/wp-content/uploads/2025/02/Key-Applications-of-Computer-Vision-in-Pothole-Detection.jpg\n",
      "2025-09-20 00:35:34,354 - INFO - downloader - image #30\thttps://i.ytimg.com/vi/TctY2cJNN0M/hq720.jpg\n",
      "2025-09-20 00:35:35,168 - ERROR - downloader - Response status code 400, file https://media.gettyimages.com/id/157743690/photo/dangerous-pot-hole-on-the-road.jpg\n",
      "2025-09-20 00:35:35,882 - ERROR - downloader - Response status code 404, file https://t3.ftcdn.net/jpg\n",
      "2025-09-20 00:35:37,129 - INFO - downloader - image #31\thttps://img.etimg.com/thumb/width-1200,height-1200,imgsize-366492,resizemode-75,msid-123933262/news/india/blackbucks-pothole-problem-ceo-rajesh-yabaji-exits-bellandur-ex-infosys-cfo-mohandas-pai-slams-govt-neglect.jpg\n",
      "2025-09-20 00:35:37,346 - ERROR - downloader - Response status code 403, file https://www.researchgate.net/publication/380087855/figure/fig2/AS:11431281285432271@1729694318361/Real-time-image-of-pothole-at-Tiruvannamalai-highway-Tamil-Nadu-India.png\n",
      "2025-09-20 00:35:37,669 - ERROR - downloader - Response status code 400, file https://media.gettyimages.com/id/1454396800/photo/a-large-pot-hole-on-residential-street-in-north-london.jpg\n",
      "2025-09-20 00:35:39,633 - INFO - downloader - image #32\thttps://people.com/thmb/fl9fV7UOeSyJU5p2kLxtGPYYhik=/4000x0/filters:no_upscale():max_bytes(150000):strip_icc():focal(749x0:751x2)/pothole-prank-022725-5-1c5ef300f6ac44f6a7fc3f01827dde70.jpg\n",
      "2025-09-20 00:35:40,770 - INFO - downloader - image #33\thttps://www.portland.gov/sites/default/files/styles/2_1_1600w/public/2023/pothole_1.jpg\n",
      "2025-09-20 00:35:41,859 - INFO - downloader - image #34\thttps://images.theconversation.com/files/489972/original/file-20221017-1357-p33k0l.jpg\n",
      "2025-09-20 00:35:42,444 - INFO - downloader - image #35\thttps://www.shutterstock.com/image-photo/deep-water-filled-pothole-shopping-260nw-2445881413.jpg\n",
      "2025-09-20 00:35:44,858 - INFO - downloader - image #36\thttps://www.claremontasphalt.com.au/wp-content/uploads/2022/01/Untitled-Facebook-Ad-1.png\n",
      "2025-09-20 00:35:45,806 - INFO - parser - parsing result page https://www.google.com/search?q=pothole&ijn=1&start=100&tbs=&tbm=isch\n",
      "Exception in thread parser-001:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python311\\Lib\\threading.py\", line 1045, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\SWARNABHA\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"C:\\Program Files\\Python311\\Lib\\threading.py\", line 982, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\SWARNABHA\\AppData\\Roaming\\Python\\Python311\\site-packages\\icrawler\\parser.py\", line 93, in worker_exec\n",
      "    for task in self.parse(response, **kwargs):\n",
      "TypeError: 'NoneType' object is not iterable\n",
      "2025-09-20 00:35:46,884 - INFO - downloader - image #37\thttps://ichef.bbci.co.uk/news/480/cpsprodpb/314b/live/c8af29f0-d411-11ef-b387-69ed9121b472.jpg\n",
      "2025-09-20 00:35:48,234 - ERROR - downloader - Response status code 400, file https://www.firstresponsefinance.co.uk/_next/image?url=https%3A%2F%2Fmedia.umbraco.io%2Ffirst-response-finance%2Fvgdlefhh%2Fcar-and-pothole-01.jpg\n",
      "2025-09-20 00:35:50,466 - INFO - downloader - image #38\thttps://betzworks.us/wp-content/uploads/2024/11/pothole-vs-sinkhole-e1730975928995.jpg\n",
      "2025-09-20 00:35:50,556 - ERROR - downloader - Response status code 401, file https://i.guim.co.uk/img/static/sys-images/Guardian/About/General/2010/3/11/1268332634782/A-car-passes-a-pothole-on-001.jpg\n",
      "2025-09-20 00:35:51,687 - INFO - downloader - image #39\thttps://static.independent.co.uk/2023/05/15/11/cornwall%20pothole%201.jpg\n",
      "2025-09-20 00:35:56,707 - INFO - downloader - no more download task for thread downloader-001\n",
      "2025-09-20 00:35:56,709 - INFO - downloader - thread downloader-001 exit\n",
      "2025-09-20 00:35:56,905 - INFO - icrawler.crawler - Crawling task done!\n"
     ]
    }
   ],
   "source": [
    "from icrawler.builtin import GoogleImageCrawler\n",
    "\n",
    "# Folder to save images\n",
    "save_folder = \"pothole_images\"\n",
    "\n",
    "crawler = GoogleImageCrawler(storage={'root_dir': save_folder})\n",
    "crawler.crawl(keyword='pothole', max_num=1000, file_idx_offset=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "358717c3-a2a2-491e-bcf6-53b8f497d531",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-20 01:25:18,369 - INFO - icrawler.crawler - start crawling...\n",
      "2025-09-20 01:25:18,369 - INFO - icrawler.crawler - starting 1 feeder threads...\n",
      "2025-09-20 01:25:18,370 - INFO - feeder - thread feeder-001 exit\n",
      "2025-09-20 01:25:18,372 - INFO - icrawler.crawler - starting 2 parser threads...\n",
      "2025-09-20 01:25:18,374 - INFO - icrawler.crawler - starting 4 downloader threads...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading images for: street garbage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-20 01:25:19,676 - INFO - parser - parsing result page https://www.google.com/search?q=street+garbage&ijn=0&start=0&tbs=&tbm=isch\n",
      "2025-09-20 01:25:19,740 - INFO - downloader - skip downloading file 000001.jpg\n",
      "2025-09-20 01:25:19,742 - INFO - downloader - skip downloading file 000002.jpg\n",
      "2025-09-20 01:25:19,744 - INFO - downloader - skip downloading file 000003.jpg\n",
      "2025-09-20 01:25:19,746 - INFO - downloader - skip downloading file 000004.jpg\n",
      "2025-09-20 01:25:19,746 - INFO - downloader - skip downloading file 000005.jpg\n",
      "2025-09-20 01:25:19,748 - INFO - downloader - skip downloading file 000006.jpg\n",
      "2025-09-20 01:25:19,748 - INFO - downloader - skip downloading file 000007.jpg\n",
      "2025-09-20 01:25:19,749 - INFO - downloader - skip downloading file 000008.jpg\n",
      "2025-09-20 01:25:19,749 - INFO - downloader - skip downloading file 000009.jpg\n",
      "2025-09-20 01:25:19,749 - INFO - downloader - skip downloading file 000010.jpg\n",
      "2025-09-20 01:25:19,749 - INFO - downloader - skip downloading file 000011.jpg\n",
      "2025-09-20 01:25:19,749 - INFO - downloader - skip downloading file 000012.jpg\n",
      "2025-09-20 01:25:19,749 - INFO - downloader - skip downloading file 000013.jpg\n",
      "2025-09-20 01:25:19,749 - INFO - downloader - skip downloading file 000014.jpg\n",
      "2025-09-20 01:25:19,753 - INFO - downloader - skip downloading file 000015.jpg\n",
      "2025-09-20 01:25:19,753 - INFO - downloader - skip downloading file 000016.jpg\n",
      "2025-09-20 01:25:19,754 - INFO - downloader - skip downloading file 000017.jpg\n",
      "2025-09-20 01:25:19,754 - INFO - downloader - skip downloading file 000018.jpg\n",
      "2025-09-20 01:25:19,755 - INFO - downloader - skip downloading file 000019.jpg\n",
      "2025-09-20 01:25:19,755 - INFO - downloader - skip downloading file 000020.jpg\n",
      "2025-09-20 01:25:19,756 - INFO - downloader - skip downloading file 000021.jpg\n",
      "2025-09-20 01:25:19,757 - INFO - downloader - skip downloading file 000022.jpg\n",
      "2025-09-20 01:25:19,757 - INFO - downloader - skip downloading file 000023.jpg\n",
      "2025-09-20 01:25:19,757 - INFO - downloader - skip downloading file 000024.jpg\n",
      "2025-09-20 01:25:19,757 - INFO - downloader - skip downloading file 000025.jpg\n",
      "2025-09-20 01:25:19,757 - INFO - downloader - skip downloading file 000026.jpg\n",
      "2025-09-20 01:25:19,757 - INFO - downloader - skip downloading file 000027.jpg\n",
      "2025-09-20 01:25:19,757 - INFO - downloader - skip downloading file 000028.jpg\n",
      "2025-09-20 01:25:19,757 - INFO - downloader - skip downloading file 000029.jpg\n",
      "2025-09-20 01:25:19,757 - INFO - downloader - skip downloading file 000030.jpg\n",
      "2025-09-20 01:25:19,762 - INFO - downloader - skip downloading file 000031.jpg\n",
      "2025-09-20 01:25:19,762 - INFO - downloader - skip downloading file 000032.jpg\n",
      "2025-09-20 01:25:19,764 - INFO - downloader - skip downloading file 000033.png\n",
      "2025-09-20 01:25:19,765 - INFO - downloader - skip downloading file 000034.jpg\n",
      "2025-09-20 01:25:19,765 - INFO - downloader - skip downloading file 000035.jpg\n",
      "2025-09-20 01:25:19,766 - INFO - downloader - skip downloading file 000036.jpg\n",
      "2025-09-20 01:25:19,766 - INFO - downloader - skip downloading file 000037.jpg\n",
      "2025-09-20 01:25:19,766 - INFO - downloader - skip downloading file 000038.jpg\n",
      "2025-09-20 01:25:19,766 - INFO - downloader - skip downloading file 000039.jpg\n",
      "2025-09-20 01:25:19,766 - INFO - downloader - skip downloading file 000040.jpg\n",
      "2025-09-20 01:25:19,766 - INFO - downloader - skip downloading file 000041.jpg\n",
      "2025-09-20 01:25:19,766 - INFO - downloader - skip downloading file 000042.jpg\n",
      "2025-09-20 01:25:19,771 - INFO - downloader - skip downloading file 000043.jpg\n",
      "2025-09-20 01:25:19,772 - INFO - downloader - skip downloading file 000044.jpg\n",
      "2025-09-20 01:25:19,772 - INFO - downloader - skip downloading file 000045.jpg\n",
      "2025-09-20 01:25:19,773 - INFO - downloader - skip downloading file 000046.jpg\n",
      "2025-09-20 01:25:19,774 - INFO - downloader - skip downloading file 000047.jpg\n",
      "2025-09-20 01:25:19,774 - INFO - downloader - skip downloading file 000048.jpg\n",
      "2025-09-20 01:25:19,775 - INFO - downloader - skip downloading file 000049.jpg\n",
      "2025-09-20 01:25:19,775 - INFO - downloader - skip downloading file 000050.jpg\n",
      "2025-09-20 01:25:20,232 - INFO - downloader - image #51\thttps://www.shutterstock.com/image-photo/haifa-israel-nov-22-garbage-260nw-2231536693.jpg\n",
      "2025-09-20 01:25:20,234 - INFO - downloader - skip downloading file 000052.jpg\n",
      "2025-09-20 01:25:20,234 - INFO - downloader - skip downloading file 000053.jpg\n",
      "2025-09-20 01:25:20,235 - INFO - downloader - skip downloading file 000054.jpg\n",
      "2025-09-20 01:25:20,235 - INFO - downloader - skip downloading file 000055.jpg\n",
      "2025-09-20 01:25:20,236 - INFO - downloader - skip downloading file 000056.jpg\n",
      "2025-09-20 01:25:20,237 - INFO - downloader - skip downloading file 000057.jpg\n",
      "2025-09-20 01:25:20,238 - INFO - downloader - skip downloading file 000058.jpg\n",
      "2025-09-20 01:25:20,238 - INFO - downloader - skip downloading file 000059.jpg\n",
      "2025-09-20 01:25:20,378 - INFO - parser - no more page urls for thread parser-002 to parse\n",
      "2025-09-20 01:25:20,380 - INFO - parser - thread parser-002 exit\n",
      "2025-09-20 01:25:20,455 - INFO - downloader - image #60\thttps://previews.123rf.com/images/idkhuanchai/idkhuanchai1909/idkhuanchai190900009/131807986-trash-and-piles-of-garbage-on-the-street.jpg\n",
      "2025-09-20 01:25:20,542 - INFO - downloader - image #61\thttps://static.toiimg.com/thumb/msid-121787099,imgsize-77414,width-400,height-225,resizemode-72/121787099.jpg\n",
      "2025-09-20 01:25:20,758 - INFO - downloader - image #62\thttps://static.toiimg.com/thumb/msid-123150477,imgsize-78140,width-400,height-225,resizemode-72/123150477.jpg\n",
      "2025-09-20 01:25:20,810 - INFO - downloader - image #63\thttps://cdn.dnaindia.com/sites/default/files/2018/07/02/699923-garbage-09.jpg\n",
      "2025-09-20 01:25:21,030 - INFO - downloader - image #64\thttps://images.theconversation.com/files/116016/original/image-20160322-32312-45ldl4.jpg\n",
      "2025-09-20 01:25:21,285 - INFO - downloader - image #65\thttps://assets.thehansindia.com/h-upload/feeds/2019/08/28/210623-north-malkajgiri.jpg\n",
      "2025-09-20 01:25:21,513 - INFO - downloader - image #66\thttps://i.ytimg.com/vi/oox4aj5wMPk/hq720.jpg\n",
      "2025-09-20 01:25:22,140 - INFO - downloader - image #67\thttps://insideclimatenews.org/wp-content/uploads/2025/06/IMG_7348.jpg\n",
      "2025-09-20 01:25:22,224 - INFO - downloader - image #68\thttps://images.stockcake.com/public/1/9/7/19781af6-d21a-4236-a61c-f4b6dd7dea9e_large/overflowing-trash-bin-stockcake.jpg\n",
      "2025-09-20 01:25:22,685 - INFO - downloader - image #69\thttps://images.moneycontrol.com/static-mcnews/2022/06/1-nepal-garbage-piles-up.jpg\n",
      "2025-09-20 01:25:22,819 - INFO - downloader - image #70\thttps://images.mid-day.com/images/images/2023/aug/Garbage-a_d.jpg\n",
      "2025-09-20 01:25:23,134 - ERROR - downloader - Response status code 404, file https://www.tribuneindia.com/sortd-service/imaginary/v22-01/jpg\n",
      "2025-09-20 01:25:23,179 - INFO - downloader - image #71\thttps://www.shutterstock.com/image-photo/bin-garbage-pile-lots-dump-600nw-1245069031.jpg\n",
      "2025-09-20 01:25:23,209 - INFO - downloader - image #72\thttps://static.vecteezy.com/system/resources/previews/055/250/929/large_2x/accumulated-garbage-lining-a-busy-city-street-demonstrating-urban-litter-crisis-photo.jpg\n",
      "2025-09-20 01:25:23,223 - ERROR - downloader - Response status code 401, file https://i.guim.co.uk/img/media/a46782f48b5f2202bbc4801281a8729aa9ccec70/0_0_8192_5464/master/8192.jpg\n",
      "2025-09-20 01:25:23,255 - ERROR - downloader - Response status code 403, file https://w.ndtvimg.com/sites/3/2022/07/22112249/swachh_bharat_mission_garbage_free_cities_istock_660x330.jpg\n",
      "2025-09-20 01:25:23,724 - INFO - downloader - image #73\thttps://static.toiimg.com/thumb/msid-122927183,width-1280,height-720,imgsize-111278,resizemode-72,overlay-toi_sw,pt-32,y_pad-40/photo.jpg\n",
      "2025-09-20 01:25:23,872 - INFO - downloader - image #74\thttps://gumlet.assettype.com/downtoearth%2F2025-02-25%2F2mfoj3tl%2FiStock-1208494778.jpg\n",
      "2025-09-20 01:25:24,071 - ERROR - downloader - Response status code 400, file https://media.gettyimages.com/id/1173807291/photo/rubbish-pollution-on-city-street.jpg\n",
      "2025-09-20 01:25:24,098 - INFO - downloader - image #75\thttps://media.newindianexpress.com/newindianexpress/2024-07/0adc4eb4-a8e8-45c2-b5a7-1d468c8768ce/Garbage.jpg\n",
      "2025-09-20 01:25:24,310 - ERROR - downloader - Response status code 403, file https://www.oneindia.com/bengaluru/kiran-mazumdar-shaw-slams-bbmp\n",
      "2025-09-20 01:25:24,456 - INFO - downloader - image #76\thttps://imagesvs.oneindia.com/img/2024/05/garbage-small-1715758489.jpg\n",
      "2025-09-20 01:25:24,768 - INFO - downloader - image #77\thttps://news.ucr.edu/sites/default/files/styles/news_article_featured_l/public/2022-02/2160px-Littering_in_Stockholm.jpg\n",
      "2025-09-20 01:25:25,831 - INFO - downloader - image #78\thttps://en.vcci.com.vn/hm_content/uploads/247-news/rac-ha-noi01-1603698318-160370-8695-2889-1603709882.jpg\n",
      "2025-09-20 01:25:26,084 - INFO - parser - no more page urls for thread parser-001 to parse\n",
      "2025-09-20 01:25:26,084 - INFO - parser - thread parser-001 exit\n",
      "2025-09-20 01:25:26,498 - INFO - downloader - image #79\thttps://cbx-prod.b-cdn.net/COLOURBOX35551837.jpg\n",
      "2025-09-20 01:25:27,177 - INFO - downloader - image #80\thttps://img.freepik.com/premium-photo/overflowing-garbage-bins-city-street-waste-management-environmental-pollution-urban-waste-disposal-recycling-bins_923559-16306.jpg\n",
      "2025-09-20 01:25:27,562 - INFO - downloader - image #81\thttps://c8.alamy.com/comp/BE3G6J/rubbish-bin-on-a-city-street-oxford-england-united-kingdom-BE3G6J.jpg\n",
      "2025-09-20 01:25:27,626 - INFO - downloader - image #82\thttps://images.hindustantimes.com/rf/image_size_960x540/HT/p2/2018/10/08/Pictures/mcd-garbage_d06a446e-cb25-11e8-a159-d4219452a912.jpg\n",
      "2025-09-20 01:25:28,118 - INFO - downloader - image #83\thttps://thumbs.dreamstime.com/b/overflowing-garbage-bin-outside-parisian-cafe-litter-scattered-cobblestone-street-highlighting-urban-waste-management-402366395.jpg\n",
      "2025-09-20 01:25:28,156 - INFO - downloader - image #84\thttps://www.thegoan.net/uploads/news/big_83861_porvorim-garbage.jpg\n",
      "2025-09-20 01:25:28,178 - INFO - downloader - image #85\thttps://cloudfront-us-east-1.images.arcpublishing.com/pmn/3BWWFDHKJNCYPG5ACPFHXWQOPU.jpg\n",
      "2025-09-20 01:25:28,677 - INFO - downloader - image #86\thttps://ebnw.net/wp-content/uploads/2025/04/image-551.png\n",
      "2025-09-20 01:25:33,124 - INFO - downloader - no more download task for thread downloader-002\n",
      "2025-09-20 01:25:33,125 - INFO - downloader - thread downloader-002 exit\n",
      "2025-09-20 01:25:33,164 - INFO - downloader - no more download task for thread downloader-001\n",
      "2025-09-20 01:25:33,165 - INFO - downloader - thread downloader-001 exit\n",
      "2025-09-20 01:25:33,191 - INFO - downloader - no more download task for thread downloader-003\n",
      "2025-09-20 01:25:33,191 - INFO - downloader - thread downloader-003 exit\n",
      "2025-09-20 01:25:33,691 - INFO - downloader - no more download task for thread downloader-004\n",
      "2025-09-20 01:25:33,692 - INFO - downloader - thread downloader-004 exit\n",
      "2025-09-20 01:25:34,384 - INFO - icrawler.crawler - Crawling task done!\n",
      "2025-09-20 01:25:34,384 - INFO - icrawler.crawler - start crawling...\n",
      "2025-09-20 01:25:34,384 - INFO - icrawler.crawler - starting 1 feeder threads...\n",
      "2025-09-20 01:25:34,385 - INFO - feeder - thread feeder-001 exit\n",
      "2025-09-20 01:25:34,386 - INFO - icrawler.crawler - starting 2 parser threads...\n",
      "2025-09-20 01:25:34,390 - INFO - icrawler.crawler - starting 4 downloader threads...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading images for: garbage dump\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-20 01:25:34,842 - INFO - parser - parsing result page https://www.google.com/search?q=garbage+dump&ijn=0&start=0&tbs=&tbm=isch\n",
      "Exception in thread parser-001:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python311\\Lib\\threading.py\", line 1045, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\SWARNABHA\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"C:\\Program Files\\Python311\\Lib\\threading.py\", line 982, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\SWARNABHA\\AppData\\Roaming\\Python\\Python311\\site-packages\\icrawler\\parser.py\", line 93, in worker_exec\n",
      "    for task in self.parse(response, **kwargs):\n",
      "TypeError: 'NoneType' object is not iterable\n",
      "2025-09-20 01:25:36,397 - INFO - parser - no more page urls for thread parser-002 to parse\n",
      "2025-09-20 01:25:36,398 - INFO - parser - thread parser-002 exit\n",
      "2025-09-20 01:25:39,401 - INFO - downloader - no more download task for thread downloader-001\n",
      "2025-09-20 01:25:39,430 - INFO - downloader - no more download task for thread downloader-002\n",
      "2025-09-20 01:25:39,430 - INFO - downloader - no more download task for thread downloader-003\n",
      "2025-09-20 01:25:39,430 - INFO - downloader - no more download task for thread downloader-004\n",
      "2025-09-20 01:25:39,432 - INFO - downloader - thread downloader-001 exit\n",
      "2025-09-20 01:25:39,432 - INFO - downloader - thread downloader-002 exit\n",
      "2025-09-20 01:25:39,439 - INFO - downloader - thread downloader-003 exit\n",
      "2025-09-20 01:25:39,440 - INFO - downloader - thread downloader-004 exit\n",
      "2025-09-20 01:25:40,394 - INFO - icrawler.crawler - Crawling task done!\n",
      "2025-09-20 01:25:40,396 - INFO - icrawler.crawler - start crawling...\n",
      "2025-09-20 01:25:40,398 - INFO - icrawler.crawler - starting 1 feeder threads...\n",
      "2025-09-20 01:25:40,402 - INFO - feeder - thread feeder-001 exit\n",
      "2025-09-20 01:25:40,405 - INFO - icrawler.crawler - starting 2 parser threads...\n",
      "2025-09-20 01:25:40,408 - INFO - icrawler.crawler - starting 4 downloader threads...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading images for: overflowing trash bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-20 01:25:40,858 - INFO - parser - parsing result page https://www.google.com/search?q=overflowing+trash+bin&ijn=0&start=0&tbs=&tbm=isch\n",
      "Exception in thread parser-001:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python311\\Lib\\threading.py\", line 1045, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\SWARNABHA\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"C:\\Program Files\\Python311\\Lib\\threading.py\", line 982, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\SWARNABHA\\AppData\\Roaming\\Python\\Python311\\site-packages\\icrawler\\parser.py\", line 93, in worker_exec\n",
      "    for task in self.parse(response, **kwargs):\n",
      "TypeError: 'NoneType' object is not iterable\n",
      "2025-09-20 01:25:42,412 - INFO - parser - no more page urls for thread parser-002 to parse\n",
      "2025-09-20 01:25:42,415 - INFO - parser - thread parser-002 exit\n",
      "2025-09-20 01:25:45,424 - INFO - downloader - no more download task for thread downloader-002\n",
      "2025-09-20 01:25:45,427 - INFO - downloader - no more download task for thread downloader-003\n",
      "2025-09-20 01:25:45,436 - INFO - downloader - thread downloader-003 exit\n",
      "2025-09-20 01:25:45,429 - INFO - downloader - no more download task for thread downloader-004\n",
      "2025-09-20 01:25:45,439 - INFO - downloader - thread downloader-004 exit\n",
      "2025-09-20 01:25:45,428 - INFO - downloader - no more download task for thread downloader-001\n",
      "2025-09-20 01:25:45,430 - INFO - downloader - thread downloader-002 exit\n",
      "2025-09-20 01:25:45,441 - INFO - downloader - thread downloader-001 exit\n",
      "2025-09-20 01:25:46,417 - INFO - icrawler.crawler - Crawling task done!\n",
      "2025-09-20 01:25:46,418 - INFO - icrawler.crawler - start crawling...\n",
      "2025-09-20 01:25:46,418 - INFO - icrawler.crawler - starting 1 feeder threads...\n",
      "2025-09-20 01:25:46,418 - INFO - feeder - thread feeder-001 exit\n",
      "2025-09-20 01:25:46,426 - INFO - icrawler.crawler - starting 2 parser threads...\n",
      "2025-09-20 01:25:46,434 - INFO - icrawler.crawler - starting 4 downloader threads...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading images for: litter on road\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-20 01:25:46,983 - INFO - parser - parsing result page https://www.google.com/search?q=litter+on+road&ijn=0&start=0&tbs=&tbm=isch\n",
      "Exception in thread parser-001:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python311\\Lib\\threading.py\", line 1045, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\SWARNABHA\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"C:\\Program Files\\Python311\\Lib\\threading.py\", line 982, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\SWARNABHA\\AppData\\Roaming\\Python\\Python311\\site-packages\\icrawler\\parser.py\", line 93, in worker_exec\n",
      "    for task in self.parse(response, **kwargs):\n",
      "TypeError: 'NoneType' object is not iterable\n",
      "2025-09-20 01:25:48,436 - INFO - parser - no more page urls for thread parser-002 to parse\n",
      "2025-09-20 01:25:48,441 - INFO - parser - thread parser-002 exit\n",
      "2025-09-20 01:25:51,447 - INFO - downloader - no more download task for thread downloader-001\n",
      "2025-09-20 01:25:51,452 - INFO - downloader - thread downloader-001 exit\n",
      "2025-09-20 01:25:51,450 - INFO - downloader - no more download task for thread downloader-003\n",
      "2025-09-20 01:25:51,450 - INFO - downloader - no more download task for thread downloader-002\n",
      "2025-09-20 01:25:51,450 - INFO - downloader - no more download task for thread downloader-004\n",
      "2025-09-20 01:25:51,456 - INFO - downloader - thread downloader-003 exit\n",
      "2025-09-20 01:25:51,482 - INFO - downloader - thread downloader-002 exit\n",
      "2025-09-20 01:25:51,487 - INFO - downloader - thread downloader-004 exit\n",
      "2025-09-20 01:25:52,443 - INFO - icrawler.crawler - Crawling task done!\n",
      "2025-09-20 01:25:52,445 - INFO - icrawler.crawler - start crawling...\n",
      "2025-09-20 01:25:52,446 - INFO - icrawler.crawler - starting 1 feeder threads...\n",
      "2025-09-20 01:25:52,448 - INFO - feeder - thread feeder-001 exit\n",
      "2025-09-20 01:25:52,452 - INFO - icrawler.crawler - starting 2 parser threads...\n",
      "2025-09-20 01:25:52,456 - INFO - icrawler.crawler - starting 4 downloader threads...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading images for: urban waste\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-20 01:25:52,921 - INFO - parser - parsing result page https://www.google.com/search?q=urban+waste&ijn=0&start=0&tbs=&tbm=isch\n",
      "Exception in thread parser-001:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python311\\Lib\\threading.py\", line 1045, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\SWARNABHA\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"C:\\Program Files\\Python311\\Lib\\threading.py\", line 982, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\SWARNABHA\\AppData\\Roaming\\Python\\Python311\\site-packages\\icrawler\\parser.py\", line 93, in worker_exec\n",
      "    for task in self.parse(response, **kwargs):\n",
      "TypeError: 'NoneType' object is not iterable\n",
      "2025-09-20 01:25:54,463 - INFO - parser - no more page urls for thread parser-002 to parse\n",
      "2025-09-20 01:25:54,465 - INFO - parser - thread parser-002 exit\n",
      "2025-09-20 01:25:57,462 - INFO - downloader - no more download task for thread downloader-001\n",
      "2025-09-20 01:25:57,464 - INFO - downloader - no more download task for thread downloader-004\n",
      "2025-09-20 01:25:57,464 - INFO - downloader - no more download task for thread downloader-003\n",
      "2025-09-20 01:25:57,464 - INFO - downloader - no more download task for thread downloader-002\n",
      "2025-09-20 01:25:57,468 - INFO - downloader - thread downloader-001 exit\n",
      "2025-09-20 01:25:57,468 - INFO - downloader - thread downloader-004 exit\n",
      "2025-09-20 01:25:57,468 - INFO - downloader - thread downloader-003 exit\n",
      "2025-09-20 01:25:57,481 - INFO - downloader - thread downloader-002 exit\n",
      "2025-09-20 01:25:58,467 - INFO - icrawler.crawler - Crawling task done!\n",
      "2025-09-20 01:25:58,469 - INFO - icrawler.crawler - start crawling...\n",
      "2025-09-20 01:25:58,470 - INFO - icrawler.crawler - starting 1 feeder threads...\n",
      "2025-09-20 01:25:58,472 - INFO - feeder - thread feeder-001 exit\n",
      "2025-09-20 01:25:58,476 - INFO - icrawler.crawler - starting 2 parser threads...\n",
      "2025-09-20 01:25:58,478 - INFO - icrawler.crawler - starting 4 downloader threads...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading images for: waste disposal site\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-20 01:25:58,974 - INFO - parser - parsing result page https://www.google.com/search?q=waste+disposal+site&ijn=0&start=0&tbs=&tbm=isch\n",
      "Exception in thread parser-001:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python311\\Lib\\threading.py\", line 1045, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\SWARNABHA\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"C:\\Program Files\\Python311\\Lib\\threading.py\", line 982, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\SWARNABHA\\AppData\\Roaming\\Python\\Python311\\site-packages\\icrawler\\parser.py\", line 93, in worker_exec\n",
      "    for task in self.parse(response, **kwargs):\n",
      "TypeError: 'NoneType' object is not iterable\n",
      "2025-09-20 01:26:00,482 - INFO - parser - no more page urls for thread parser-002 to parse\n",
      "2025-09-20 01:26:00,487 - INFO - parser - thread parser-002 exit\n",
      "2025-09-20 01:26:03,492 - INFO - downloader - no more download task for thread downloader-001\n",
      "2025-09-20 01:26:03,496 - INFO - downloader - thread downloader-001 exit\n",
      "2025-09-20 01:26:03,497 - INFO - downloader - no more download task for thread downloader-003\n",
      "2025-09-20 01:26:03,497 - INFO - downloader - no more download task for thread downloader-004\n",
      "2025-09-20 01:26:03,499 - INFO - downloader - no more download task for thread downloader-002\n",
      "2025-09-20 01:26:03,525 - INFO - downloader - thread downloader-002 exit\n",
      "2025-09-20 01:26:03,522 - INFO - downloader - thread downloader-004 exit\n",
      "2025-09-20 01:26:03,503 - INFO - downloader - thread downloader-003 exit\n",
      "2025-09-20 01:26:04,484 - INFO - icrawler.crawler - Crawling task done!\n",
      "2025-09-20 01:26:04,484 - INFO - icrawler.crawler - start crawling...\n",
      "2025-09-20 01:26:04,484 - INFO - icrawler.crawler - starting 1 feeder threads...\n",
      "2025-09-20 01:26:04,486 - INFO - feeder - thread feeder-001 exit\n",
      "2025-09-20 01:26:04,487 - INFO - icrawler.crawler - starting 2 parser threads...\n",
      "2025-09-20 01:26:04,488 - INFO - icrawler.crawler - starting 4 downloader threads...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading images for: public trash pile\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-20 01:26:05,296 - INFO - parser - parsing result page https://www.google.com/search?q=public+trash+pile&ijn=0&start=0&tbs=&tbm=isch\n",
      "Exception in thread parser-001:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python311\\Lib\\threading.py\", line 1045, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\SWARNABHA\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"C:\\Program Files\\Python311\\Lib\\threading.py\", line 982, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\SWARNABHA\\AppData\\Roaming\\Python\\Python311\\site-packages\\icrawler\\parser.py\", line 93, in worker_exec\n",
      "    for task in self.parse(response, **kwargs):\n",
      "TypeError: 'NoneType' object is not iterable\n",
      "2025-09-20 01:26:06,497 - INFO - parser - no more page urls for thread parser-002 to parse\n",
      "2025-09-20 01:26:06,497 - INFO - parser - thread parser-002 exit\n",
      "2025-09-20 01:26:09,492 - INFO - downloader - no more download task for thread downloader-003\n",
      "2025-09-20 01:26:09,510 - INFO - downloader - thread downloader-003 exit\n",
      "2025-09-20 01:26:09,510 - INFO - downloader - no more download task for thread downloader-004\n",
      "2025-09-20 01:26:09,510 - INFO - downloader - no more download task for thread downloader-001\n",
      "2025-09-20 01:26:09,535 - INFO - downloader - thread downloader-001 exit\n",
      "2025-09-20 01:26:09,516 - INFO - downloader - thread downloader-004 exit\n",
      "2025-09-20 01:26:09,510 - INFO - downloader - no more download task for thread downloader-002\n",
      "2025-09-20 01:26:09,539 - INFO - downloader - thread downloader-002 exit\n",
      "2025-09-20 01:26:10,512 - INFO - icrawler.crawler - Crawling task done!\n",
      "2025-09-20 01:26:10,514 - INFO - icrawler.crawler - start crawling...\n",
      "2025-09-20 01:26:10,516 - INFO - icrawler.crawler - starting 1 feeder threads...\n",
      "2025-09-20 01:26:10,518 - INFO - feeder - thread feeder-001 exit\n",
      "2025-09-20 01:26:10,525 - INFO - icrawler.crawler - starting 2 parser threads...\n",
      "2025-09-20 01:26:10,528 - INFO - icrawler.crawler - starting 4 downloader threads...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading images for: roadside garbage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-20 01:26:10,918 - INFO - parser - parsing result page https://www.google.com/search?q=roadside+garbage&ijn=0&start=0&tbs=&tbm=isch\n",
      "Exception in thread parser-001:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python311\\Lib\\threading.py\", line 1045, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\SWARNABHA\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"C:\\Program Files\\Python311\\Lib\\threading.py\", line 982, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\SWARNABHA\\AppData\\Roaming\\Python\\Python311\\site-packages\\icrawler\\parser.py\", line 93, in worker_exec\n",
      "    for task in self.parse(response, **kwargs):\n",
      "TypeError: 'NoneType' object is not iterable\n",
      "2025-09-20 01:26:12,535 - INFO - parser - no more page urls for thread parser-002 to parse\n",
      "2025-09-20 01:26:12,537 - INFO - parser - thread parser-002 exit\n",
      "2025-09-20 01:26:15,531 - INFO - downloader - no more download task for thread downloader-001\n",
      "2025-09-20 01:26:15,534 - INFO - downloader - no more download task for thread downloader-002\n",
      "2025-09-20 01:26:15,534 - INFO - downloader - no more download task for thread downloader-003\n",
      "2025-09-20 01:26:15,541 - INFO - downloader - thread downloader-003 exit\n",
      "2025-09-20 01:26:15,539 - INFO - downloader - thread downloader-002 exit\n",
      "2025-09-20 01:26:15,540 - INFO - downloader - no more download task for thread downloader-004\n",
      "2025-09-20 01:26:15,535 - INFO - downloader - thread downloader-001 exit\n",
      "2025-09-20 01:26:15,545 - INFO - downloader - thread downloader-004 exit\n",
      "2025-09-20 01:26:16,536 - INFO - icrawler.crawler - Crawling task done!\n",
      "2025-09-20 01:26:16,536 - INFO - icrawler.crawler - start crawling...\n",
      "2025-09-20 01:26:16,536 - INFO - icrawler.crawler - starting 1 feeder threads...\n",
      "2025-09-20 01:26:16,536 - INFO - feeder - thread feeder-001 exit\n",
      "2025-09-20 01:26:16,540 - INFO - icrawler.crawler - starting 2 parser threads...\n",
      "2025-09-20 01:26:16,541 - INFO - icrawler.crawler - starting 4 downloader threads...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading images for: plastic waste street\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-20 01:26:17,014 - INFO - parser - parsing result page https://www.google.com/search?q=plastic+waste+street&ijn=0&start=0&tbs=&tbm=isch\n",
      "Exception in thread parser-001:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python311\\Lib\\threading.py\", line 1045, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\SWARNABHA\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"C:\\Program Files\\Python311\\Lib\\threading.py\", line 982, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\SWARNABHA\\AppData\\Roaming\\Python\\Python311\\site-packages\\icrawler\\parser.py\", line 93, in worker_exec\n",
      "    for task in self.parse(response, **kwargs):\n",
      "TypeError: 'NoneType' object is not iterable\n",
      "2025-09-20 01:26:18,554 - INFO - parser - no more page urls for thread parser-002 to parse\n",
      "2025-09-20 01:26:18,557 - INFO - parser - thread parser-002 exit\n",
      "2025-09-20 01:26:21,547 - INFO - downloader - no more download task for thread downloader-001\n",
      "2025-09-20 01:26:21,553 - INFO - downloader - thread downloader-001 exit\n",
      "2025-09-20 01:26:21,551 - INFO - downloader - no more download task for thread downloader-004\n",
      "2025-09-20 01:26:21,552 - INFO - downloader - no more download task for thread downloader-002\n",
      "2025-09-20 01:26:21,551 - INFO - downloader - no more download task for thread downloader-003\n",
      "2025-09-20 01:26:21,561 - INFO - downloader - thread downloader-003 exit\n",
      "2025-09-20 01:26:21,559 - INFO - downloader - thread downloader-002 exit\n",
      "2025-09-20 01:26:21,557 - INFO - downloader - thread downloader-004 exit\n",
      "2025-09-20 01:26:22,552 - INFO - icrawler.crawler - Crawling task done!\n",
      "2025-09-20 01:26:22,554 - INFO - icrawler.crawler - start crawling...\n",
      "2025-09-20 01:26:22,556 - INFO - icrawler.crawler - starting 1 feeder threads...\n",
      "2025-09-20 01:26:22,556 - INFO - feeder - thread feeder-001 exit\n",
      "2025-09-20 01:26:22,562 - INFO - icrawler.crawler - starting 2 parser threads...\n",
      "2025-09-20 01:26:22,564 - INFO - icrawler.crawler - starting 4 downloader threads...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading images for: illegal dumping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-20 01:26:22,970 - INFO - parser - parsing result page https://www.google.com/search?q=illegal+dumping&ijn=0&start=0&tbs=&tbm=isch\n",
      "Exception in thread parser-001:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python311\\Lib\\threading.py\", line 1045, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\SWARNABHA\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"C:\\Program Files\\Python311\\Lib\\threading.py\", line 982, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\SWARNABHA\\AppData\\Roaming\\Python\\Python311\\site-packages\\icrawler\\parser.py\", line 93, in worker_exec\n",
      "    for task in self.parse(response, **kwargs):\n",
      "TypeError: 'NoneType' object is not iterable\n",
      "2025-09-20 01:26:24,573 - INFO - parser - no more page urls for thread parser-002 to parse\n",
      "2025-09-20 01:26:24,576 - INFO - parser - thread parser-002 exit\n",
      "2025-09-20 01:26:27,569 - INFO - downloader - no more download task for thread downloader-001\n",
      "2025-09-20 01:26:27,570 - INFO - downloader - no more download task for thread downloader-004\n",
      "2025-09-20 01:26:27,570 - INFO - downloader - no more download task for thread downloader-003\n",
      "2025-09-20 01:26:27,570 - INFO - downloader - no more download task for thread downloader-002\n",
      "2025-09-20 01:26:27,584 - INFO - downloader - thread downloader-002 exit\n",
      "2025-09-20 01:26:27,578 - INFO - downloader - thread downloader-004 exit\n",
      "2025-09-20 01:26:27,578 - INFO - downloader - thread downloader-003 exit\n",
      "2025-09-20 01:26:27,570 - INFO - downloader - thread downloader-001 exit\n",
      "2025-09-20 01:26:28,574 - INFO - icrawler.crawler - Crawling task done!\n"
     ]
    }
   ],
   "source": [
    "from icrawler.builtin import GoogleImageCrawler\n",
    "from icrawler import ImageDownloader\n",
    "\n",
    "# Custom downloader with headers to avoid 403\n",
    "class MyDownloader(ImageDownloader):\n",
    "    def download(self, task, default_ext, timeout=5, **kwargs):\n",
    "        self.session.headers.update({'User-Agent': 'Mozilla/5.0'})\n",
    "        return super().download(task, default_ext, timeout=timeout, **kwargs)\n",
    "\n",
    "# Create crawler with custom downloader\n",
    "crawler = GoogleImageCrawler(\n",
    "    storage={'root_dir': r\"C:/garbage_img\"},\n",
    "    feeder_threads=1,\n",
    "    parser_threads=2,\n",
    "    downloader_threads=4,\n",
    "    downloader_cls=MyDownloader\n",
    ")\n",
    "\n",
    "garbage_keywords = [\n",
    "    \"street garbage\",\n",
    "    \"garbage dump\",\n",
    "    \"overflowing trash bin\",\n",
    "    \"litter on road\",\n",
    "    \"urban waste\",\n",
    "    \"waste disposal site\",\n",
    "    \"public trash pile\",\n",
    "    \"roadside garbage\",\n",
    "    \"plastic waste street\",\n",
    "    \"illegal dumping\"\n",
    "]\n",
    "\n",
    "# Crawl for each keyword\n",
    "for i, keyword in enumerate(garbage_keywords):\n",
    "    print(f\"Downloading images for: {keyword}\")\n",
    "    crawler.crawl(\n",
    "        keyword=keyword,\n",
    "        max_num=100,   # You can set 100 per keyword to get ~1000 total\n",
    "        file_idx_offset=i*100\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "867de3f9-95ae-4af6-84de-f01f9959a8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-20 01:30:01,073 - INFO - icrawler.crawler - start crawling...\n",
      "2025-09-20 01:30:01,074 - INFO - icrawler.crawler - starting 1 feeder threads...\n",
      "2025-09-20 01:30:01,075 - INFO - feeder - thread feeder-001 exit\n",
      "2025-09-20 01:30:01,078 - INFO - icrawler.crawler - starting 2 parser threads...\n",
      "2025-09-20 01:30:01,079 - INFO - icrawler.crawler - starting 4 downloader threads...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading images for: street garbage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-20 01:30:02,311 - INFO - parser - parsing result page https://www.google.com/search?q=street+garbage&ijn=0&start=0&tbs=&tbm=isch\n",
      "2025-09-20 01:30:02,377 - INFO - downloader - skip downloading file 000001.jpg\n",
      "2025-09-20 01:30:02,377 - INFO - downloader - skip downloading file 000002.jpg\n",
      "2025-09-20 01:30:02,377 - INFO - downloader - skip downloading file 000003.jpg\n",
      "2025-09-20 01:30:02,382 - INFO - downloader - skip downloading file 000004.jpg\n",
      "2025-09-20 01:30:02,383 - INFO - downloader - skip downloading file 000005.jpg\n",
      "2025-09-20 01:30:02,384 - INFO - downloader - skip downloading file 000006.jpg\n",
      "2025-09-20 01:30:02,384 - INFO - downloader - skip downloading file 000007.jpg\n",
      "2025-09-20 01:30:02,386 - INFO - downloader - skip downloading file 000008.jpg\n",
      "2025-09-20 01:30:02,387 - INFO - downloader - skip downloading file 000009.jpg\n",
      "2025-09-20 01:30:02,388 - INFO - downloader - skip downloading file 000010.jpg\n",
      "2025-09-20 01:30:02,388 - INFO - downloader - skip downloading file 000011.jpg\n",
      "2025-09-20 01:30:02,388 - INFO - downloader - skip downloading file 000012.jpg\n",
      "2025-09-20 01:30:02,388 - INFO - downloader - skip downloading file 000013.jpg\n",
      "2025-09-20 01:30:02,388 - INFO - downloader - skip downloading file 000014.jpg\n",
      "2025-09-20 01:30:02,388 - INFO - downloader - skip downloading file 000015.jpg\n",
      "2025-09-20 01:30:02,388 - INFO - downloader - skip downloading file 000016.jpg\n",
      "2025-09-20 01:30:02,388 - INFO - downloader - skip downloading file 000017.jpg\n",
      "2025-09-20 01:30:02,388 - INFO - downloader - skip downloading file 000018.jpg\n",
      "2025-09-20 01:30:02,393 - INFO - downloader - skip downloading file 000019.jpg\n",
      "2025-09-20 01:30:02,394 - INFO - downloader - skip downloading file 000020.jpg\n",
      "2025-09-20 01:30:02,394 - INFO - downloader - skip downloading file 000021.jpg\n",
      "2025-09-20 01:30:02,395 - INFO - downloader - skip downloading file 000022.jpg\n",
      "2025-09-20 01:30:02,395 - INFO - downloader - skip downloading file 000023.jpg\n",
      "2025-09-20 01:30:02,396 - INFO - downloader - skip downloading file 000024.jpg\n",
      "2025-09-20 01:30:02,396 - INFO - downloader - skip downloading file 000025.jpg\n",
      "2025-09-20 01:30:02,397 - INFO - downloader - skip downloading file 000026.jpg\n",
      "2025-09-20 01:30:02,398 - INFO - downloader - skip downloading file 000027.jpg\n",
      "2025-09-20 01:30:02,399 - INFO - downloader - skip downloading file 000028.jpg\n",
      "2025-09-20 01:30:02,400 - INFO - downloader - skip downloading file 000029.jpg\n",
      "2025-09-20 01:30:02,400 - INFO - downloader - skip downloading file 000030.jpg\n",
      "2025-09-20 01:30:02,401 - INFO - downloader - skip downloading file 000031.jpg\n",
      "2025-09-20 01:30:02,401 - INFO - downloader - skip downloading file 000032.jpg\n",
      "2025-09-20 01:30:02,402 - INFO - downloader - skip downloading file 000033.jpg\n",
      "2025-09-20 01:30:02,402 - INFO - downloader - skip downloading file 000034.jpg\n",
      "2025-09-20 01:30:02,403 - INFO - downloader - skip downloading file 000035.jpg\n",
      "2025-09-20 01:30:02,403 - INFO - downloader - skip downloading file 000036.jpg\n",
      "2025-09-20 01:30:02,404 - INFO - downloader - skip downloading file 000037.jpg\n",
      "2025-09-20 01:30:02,405 - INFO - downloader - skip downloading file 000038.jpg\n",
      "2025-09-20 01:30:02,405 - INFO - downloader - skip downloading file 000039.jpg\n",
      "2025-09-20 01:30:02,406 - INFO - downloader - skip downloading file 000040.jpg\n",
      "2025-09-20 01:30:02,406 - INFO - downloader - skip downloading file 000041.jpg\n",
      "2025-09-20 01:30:02,407 - INFO - downloader - skip downloading file 000042.jpg\n",
      "2025-09-20 01:30:02,407 - INFO - downloader - skip downloading file 000043.jpg\n",
      "2025-09-20 01:30:02,408 - INFO - downloader - skip downloading file 000044.jpg\n",
      "2025-09-20 01:30:02,408 - INFO - downloader - skip downloading file 000045.jpg\n",
      "2025-09-20 01:30:02,409 - INFO - downloader - skip downloading file 000046.jpg\n",
      "2025-09-20 01:30:02,409 - INFO - downloader - skip downloading file 000047.jpg\n",
      "2025-09-20 01:30:02,410 - INFO - downloader - skip downloading file 000048.jpg\n",
      "2025-09-20 01:30:02,410 - INFO - downloader - skip downloading file 000049.jpg\n",
      "2025-09-20 01:30:02,411 - INFO - downloader - skip downloading file 000050.jpg\n",
      "2025-09-20 01:30:02,411 - INFO - downloader - skip downloading file 000051.jpg\n",
      "2025-09-20 01:30:02,412 - INFO - downloader - skip downloading file 000052.jpg\n",
      "2025-09-20 01:30:02,413 - INFO - downloader - skip downloading file 000053.jpg\n",
      "2025-09-20 01:30:02,413 - INFO - downloader - skip downloading file 000054.jpg\n",
      "2025-09-20 01:30:02,414 - INFO - downloader - skip downloading file 000055.jpg\n",
      "2025-09-20 01:30:02,415 - INFO - downloader - skip downloading file 000056.jpg\n",
      "2025-09-20 01:30:02,415 - INFO - downloader - skip downloading file 000057.jpg\n",
      "2025-09-20 01:30:02,416 - INFO - downloader - skip downloading file 000058.jpg\n",
      "2025-09-20 01:30:02,416 - INFO - downloader - skip downloading file 000059.jpg\n",
      "2025-09-20 01:30:02,417 - INFO - downloader - skip downloading file 000060.jpg\n",
      "2025-09-20 01:30:02,417 - INFO - downloader - skip downloading file 000061.jpg\n",
      "2025-09-20 01:30:02,418 - INFO - downloader - skip downloading file 000062.jpg\n",
      "2025-09-20 01:30:02,418 - INFO - downloader - skip downloading file 000063.jpg\n",
      "2025-09-20 01:30:02,419 - INFO - downloader - skip downloading file 000064.jpg\n",
      "2025-09-20 01:30:02,420 - INFO - downloader - skip downloading file 000065.jpg\n",
      "2025-09-20 01:30:02,420 - ERROR - downloader - Response status code 403, file https://preview.redd.it/a-street-in-paris-after-weeks-of-garbage-collector-strikes-v0-16rkh65n3qpa1.png\n",
      "2025-09-20 01:30:02,420 - INFO - downloader - skip downloading file 000066.jpg\n",
      "2025-09-20 01:30:02,421 - INFO - downloader - skip downloading file 000067.jpg\n",
      "2025-09-20 01:30:02,421 - INFO - downloader - skip downloading file 000068.jpg\n",
      "2025-09-20 01:30:02,421 - INFO - downloader - skip downloading file 000069.jpg\n",
      "2025-09-20 01:30:02,421 - INFO - downloader - skip downloading file 000070.jpg\n",
      "2025-09-20 01:30:02,421 - INFO - downloader - skip downloading file 000071.jpg\n",
      "2025-09-20 01:30:02,421 - INFO - downloader - skip downloading file 000072.jpg\n",
      "2025-09-20 01:30:02,421 - INFO - downloader - skip downloading file 000073.jpg\n",
      "2025-09-20 01:30:02,426 - INFO - downloader - skip downloading file 000074.jpg\n",
      "2025-09-20 01:30:02,426 - INFO - downloader - skip downloading file 000075.jpg\n",
      "2025-09-20 01:30:02,426 - INFO - downloader - skip downloading file 000076.jpg\n",
      "2025-09-20 01:30:02,427 - INFO - downloader - skip downloading file 000077.jpg\n",
      "2025-09-20 01:30:02,427 - INFO - downloader - skip downloading file 000078.jpg\n",
      "2025-09-20 01:30:02,428 - INFO - downloader - skip downloading file 000079.jpg\n",
      "2025-09-20 01:30:02,429 - INFO - downloader - skip downloading file 000080.jpg\n",
      "2025-09-20 01:30:02,429 - INFO - downloader - skip downloading file 000081.jpg\n",
      "2025-09-20 01:30:02,429 - INFO - downloader - skip downloading file 000082.jpg\n",
      "2025-09-20 01:30:02,429 - INFO - downloader - skip downloading file 000083.jpg\n",
      "2025-09-20 01:30:02,429 - INFO - downloader - skip downloading file 000084.jpg\n",
      "2025-09-20 01:30:02,429 - INFO - downloader - skip downloading file 000085.jpg\n",
      "2025-09-20 01:30:02,488 - INFO - downloader - image #86\thttps://images.hindustantimes.com/rf/image_size_960x540/HT/p2/2018/10/08/Pictures/mcd-garbage_d06a446e-cb25-11e8-a159-d4219452a912.jpg\n",
      "2025-09-20 01:30:02,507 - INFO - downloader - image #87\thttps://news.ucr.edu/sites/default/files/styles/news_article_featured_l/public/2022-02/2160px-Littering_in_Stockholm.jpg\n",
      "2025-09-20 01:30:02,519 - INFO - downloader - image #88\thttps://c8.alamy.com/comp/BE3G6J/rubbish-bin-on-a-city-street-oxford-england-united-kingdom-BE3G6J.jpg\n",
      "2025-09-20 01:30:02,735 - INFO - downloader - image #89\thttps://thumbs.dreamstime.com/b/overflowing-garbage-bin-outside-parisian-cafe-litter-scattered-cobblestone-street-highlighting-urban-waste-management-402366395.jpg\n",
      "2025-09-20 01:30:02,761 - INFO - downloader - image #90\thttps://www.thegoan.net/uploads/news/big_83861_porvorim-garbage.jpg\n",
      "2025-09-20 01:30:03,087 - INFO - parser - no more page urls for thread parser-002 to parse\n",
      "2025-09-20 01:30:03,090 - INFO - parser - thread parser-002 exit\n",
      "2025-09-20 01:30:03,197 - INFO - downloader - image #91\thttps://ebnw.net/wp-content/uploads/2025/04/image-551.png\n",
      "2025-09-20 01:30:04,431 - INFO - parser - no more page urls for thread parser-001 to parse\n",
      "2025-09-20 01:30:04,432 - INFO - parser - thread parser-001 exit\n",
      "2025-09-20 01:30:07,533 - INFO - downloader - no more download task for thread downloader-001\n",
      "2025-09-20 01:30:07,534 - INFO - downloader - thread downloader-001 exit\n",
      "2025-09-20 01:30:07,741 - INFO - downloader - no more download task for thread downloader-003\n",
      "2025-09-20 01:30:07,741 - INFO - downloader - thread downloader-003 exit\n",
      "2025-09-20 01:30:07,766 - INFO - downloader - no more download task for thread downloader-002\n",
      "2025-09-20 01:30:07,767 - INFO - downloader - thread downloader-002 exit\n",
      "2025-09-20 01:30:08,208 - INFO - downloader - no more download task for thread downloader-004\n",
      "2025-09-20 01:30:08,209 - INFO - downloader - thread downloader-004 exit\n",
      "2025-09-20 01:30:09,085 - INFO - icrawler.crawler - Crawling task done!\n",
      "2025-09-20 01:30:09,086 - INFO - icrawler.crawler - start crawling...\n",
      "2025-09-20 01:30:09,087 - INFO - icrawler.crawler - starting 1 feeder threads...\n",
      "2025-09-20 01:30:09,089 - INFO - feeder - thread feeder-001 exit\n",
      "2025-09-20 01:30:09,106 - INFO - icrawler.crawler - starting 2 parser threads...\n",
      "2025-09-20 01:30:09,110 - INFO - icrawler.crawler - starting 4 downloader threads...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading images for: garbage dump\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-20 01:30:09,425 - INFO - parser - parsing result page https://www.google.com/search?q=garbage+dump&ijn=0&start=0&tbs=&tbm=isch\n",
      "Exception in thread parser-001:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python311\\Lib\\threading.py\", line 1045, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\SWARNABHA\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"C:\\Program Files\\Python311\\Lib\\threading.py\", line 982, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\SWARNABHA\\AppData\\Roaming\\Python\\Python311\\site-packages\\icrawler\\parser.py\", line 93, in worker_exec\n",
      "    for task in self.parse(response, **kwargs):\n",
      "TypeError: 'NoneType' object is not iterable\n",
      "2025-09-20 01:30:11,114 - INFO - parser - no more page urls for thread parser-002 to parse\n",
      "2025-09-20 01:30:11,117 - INFO - parser - thread parser-002 exit\n",
      "2025-09-20 01:30:14,122 - INFO - downloader - no more download task for thread downloader-004\n",
      "2025-09-20 01:30:14,140 - INFO - downloader - thread downloader-004 exit\n",
      "2025-09-20 01:30:14,140 - INFO - downloader - no more download task for thread downloader-001\n",
      "2025-09-20 01:30:14,140 - INFO - downloader - no more download task for thread downloader-002\n",
      "2025-09-20 01:30:14,140 - INFO - downloader - no more download task for thread downloader-003\n",
      "2025-09-20 01:30:14,159 - INFO - downloader - thread downloader-003 exit\n",
      "2025-09-20 01:30:14,157 - INFO - downloader - thread downloader-002 exit\n",
      "2025-09-20 01:30:14,146 - INFO - downloader - thread downloader-001 exit\n",
      "2025-09-20 01:30:15,119 - INFO - icrawler.crawler - Crawling task done!\n",
      "2025-09-20 01:30:15,122 - INFO - icrawler.crawler - start crawling...\n",
      "2025-09-20 01:30:15,123 - INFO - icrawler.crawler - starting 1 feeder threads...\n",
      "2025-09-20 01:30:15,125 - INFO - feeder - thread feeder-001 exit\n",
      "2025-09-20 01:30:15,128 - INFO - icrawler.crawler - starting 2 parser threads...\n",
      "2025-09-20 01:30:15,132 - INFO - icrawler.crawler - starting 4 downloader threads...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading images for: overflowing trash bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-20 01:30:15,434 - INFO - parser - parsing result page https://www.google.com/search?q=overflowing+trash+bin&ijn=0&start=0&tbs=&tbm=isch\n",
      "Exception in thread parser-001:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python311\\Lib\\threading.py\", line 1045, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\SWARNABHA\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"C:\\Program Files\\Python311\\Lib\\threading.py\", line 982, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\SWARNABHA\\AppData\\Roaming\\Python\\Python311\\site-packages\\icrawler\\parser.py\", line 93, in worker_exec\n",
      "    for task in self.parse(response, **kwargs):\n",
      "TypeError: 'NoneType' object is not iterable\n",
      "2025-09-20 01:30:17,146 - INFO - parser - no more page urls for thread parser-002 to parse\n",
      "2025-09-20 01:30:17,148 - INFO - parser - thread parser-002 exit\n",
      "2025-09-20 01:30:20,135 - INFO - downloader - no more download task for thread downloader-002\n",
      "2025-09-20 01:30:20,138 - INFO - downloader - thread downloader-002 exit\n",
      "2025-09-20 01:30:20,137 - INFO - downloader - no more download task for thread downloader-004\n",
      "2025-09-20 01:30:20,138 - INFO - downloader - no more download task for thread downloader-003\n",
      "2025-09-20 01:30:20,138 - INFO - downloader - no more download task for thread downloader-001\n",
      "2025-09-20 01:30:20,142 - INFO - downloader - thread downloader-004 exit\n",
      "2025-09-20 01:30:20,146 - INFO - downloader - thread downloader-003 exit\n",
      "2025-09-20 01:30:20,147 - INFO - downloader - thread downloader-001 exit\n",
      "2025-09-20 01:30:21,138 - INFO - icrawler.crawler - Crawling task done!\n",
      "2025-09-20 01:30:21,139 - INFO - icrawler.crawler - start crawling...\n",
      "2025-09-20 01:30:21,141 - INFO - icrawler.crawler - starting 1 feeder threads...\n",
      "2025-09-20 01:30:21,143 - INFO - feeder - thread feeder-001 exit\n",
      "2025-09-20 01:30:21,144 - INFO - icrawler.crawler - starting 2 parser threads...\n",
      "2025-09-20 01:30:21,147 - INFO - icrawler.crawler - starting 4 downloader threads...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading images for: litter on road\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-20 01:30:21,433 - INFO - parser - parsing result page https://www.google.com/search?q=litter+on+road&ijn=0&start=0&tbs=&tbm=isch\n",
      "Exception in thread parser-001:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python311\\Lib\\threading.py\", line 1045, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\SWARNABHA\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"C:\\Program Files\\Python311\\Lib\\threading.py\", line 982, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\SWARNABHA\\AppData\\Roaming\\Python\\Python311\\site-packages\\icrawler\\parser.py\", line 93, in worker_exec\n",
      "    for task in self.parse(response, **kwargs):\n",
      "TypeError: 'NoneType' object is not iterable\n",
      "2025-09-20 01:30:23,154 - INFO - parser - no more page urls for thread parser-002 to parse\n",
      "2025-09-20 01:30:23,170 - INFO - parser - thread parser-002 exit\n",
      "2025-09-20 01:30:26,150 - INFO - downloader - no more download task for thread downloader-001\n",
      "2025-09-20 01:30:26,166 - INFO - downloader - no more download task for thread downloader-002\n",
      "2025-09-20 01:30:26,166 - INFO - downloader - no more download task for thread downloader-003\n",
      "2025-09-20 01:30:26,166 - INFO - downloader - no more download task for thread downloader-004\n",
      "2025-09-20 01:30:26,166 - INFO - downloader - thread downloader-001 exit\n",
      "2025-09-20 01:30:26,168 - INFO - downloader - thread downloader-002 exit\n",
      "2025-09-20 01:30:26,168 - INFO - downloader - thread downloader-003 exit\n",
      "2025-09-20 01:30:26,168 - INFO - downloader - thread downloader-004 exit\n",
      "2025-09-20 01:30:27,167 - INFO - icrawler.crawler - Crawling task done!\n",
      "2025-09-20 01:30:27,170 - INFO - icrawler.crawler - start crawling...\n",
      "2025-09-20 01:30:27,172 - INFO - icrawler.crawler - starting 1 feeder threads...\n",
      "2025-09-20 01:30:27,174 - INFO - feeder - thread feeder-001 exit\n",
      "2025-09-20 01:30:27,177 - INFO - icrawler.crawler - starting 2 parser threads...\n",
      "2025-09-20 01:30:27,181 - INFO - icrawler.crawler - starting 4 downloader threads...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading images for: urban waste\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-20 01:30:27,493 - INFO - parser - parsing result page https://www.google.com/search?q=urban+waste&ijn=0&start=0&tbs=&tbm=isch\n",
      "Exception in thread parser-001:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python311\\Lib\\threading.py\", line 1045, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\SWARNABHA\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"C:\\Program Files\\Python311\\Lib\\threading.py\", line 982, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\SWARNABHA\\AppData\\Roaming\\Python\\Python311\\site-packages\\icrawler\\parser.py\", line 93, in worker_exec\n",
      "    for task in self.parse(response, **kwargs):\n",
      "TypeError: 'NoneType' object is not iterable\n",
      "2025-09-20 01:30:29,183 - INFO - parser - no more page urls for thread parser-002 to parse\n",
      "2025-09-20 01:30:29,201 - INFO - parser - thread parser-002 exit\n",
      "2025-09-20 01:30:32,198 - INFO - downloader - no more download task for thread downloader-001\n",
      "2025-09-20 01:30:32,218 - INFO - downloader - thread downloader-001 exit\n",
      "2025-09-20 01:30:32,217 - INFO - downloader - no more download task for thread downloader-003\n",
      "2025-09-20 01:30:32,217 - INFO - downloader - no more download task for thread downloader-002\n",
      "2025-09-20 01:30:32,217 - INFO - downloader - no more download task for thread downloader-004\n",
      "2025-09-20 01:30:32,224 - INFO - downloader - thread downloader-004 exit\n",
      "2025-09-20 01:30:32,221 - INFO - downloader - thread downloader-002 exit\n",
      "2025-09-20 01:30:32,220 - INFO - downloader - thread downloader-003 exit\n",
      "2025-09-20 01:30:33,218 - INFO - icrawler.crawler - Crawling task done!\n",
      "2025-09-20 01:30:33,220 - INFO - icrawler.crawler - start crawling...\n",
      "2025-09-20 01:30:33,221 - INFO - icrawler.crawler - starting 1 feeder threads...\n",
      "2025-09-20 01:30:33,224 - INFO - feeder - thread feeder-001 exit\n",
      "2025-09-20 01:30:33,227 - INFO - icrawler.crawler - starting 2 parser threads...\n",
      "2025-09-20 01:30:33,231 - INFO - icrawler.crawler - starting 4 downloader threads...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading images for: waste disposal site\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-20 01:30:33,550 - INFO - parser - parsing result page https://www.google.com/search?q=waste+disposal+site&ijn=0&start=0&tbs=&tbm=isch\n",
      "Exception in thread parser-001:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python311\\Lib\\threading.py\", line 1045, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\SWARNABHA\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"C:\\Program Files\\Python311\\Lib\\threading.py\", line 982, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\SWARNABHA\\AppData\\Roaming\\Python\\Python311\\site-packages\\icrawler\\parser.py\", line 93, in worker_exec\n",
      "    for task in self.parse(response, **kwargs):\n",
      "TypeError: 'NoneType' object is not iterable\n",
      "2025-09-20 01:30:35,247 - INFO - parser - no more page urls for thread parser-002 to parse\n",
      "2025-09-20 01:30:35,247 - INFO - parser - thread parser-002 exit\n",
      "2025-09-20 01:30:38,244 - INFO - downloader - no more download task for thread downloader-002\n",
      "2025-09-20 01:30:38,249 - INFO - downloader - thread downloader-002 exit\n",
      "2025-09-20 01:30:38,248 - INFO - downloader - no more download task for thread downloader-003\n",
      "2025-09-20 01:30:38,248 - INFO - downloader - no more download task for thread downloader-001\n",
      "2025-09-20 01:30:38,248 - INFO - downloader - no more download task for thread downloader-004\n",
      "2025-09-20 01:30:38,253 - INFO - downloader - thread downloader-003 exit\n",
      "2025-09-20 01:30:38,255 - INFO - downloader - thread downloader-001 exit\n",
      "2025-09-20 01:30:38,257 - INFO - downloader - thread downloader-004 exit\n",
      "2025-09-20 01:30:39,236 - INFO - icrawler.crawler - Crawling task done!\n",
      "2025-09-20 01:30:39,236 - INFO - icrawler.crawler - start crawling...\n",
      "2025-09-20 01:30:39,237 - INFO - icrawler.crawler - starting 1 feeder threads...\n",
      "2025-09-20 01:30:39,238 - INFO - feeder - thread feeder-001 exit\n",
      "2025-09-20 01:30:39,239 - INFO - icrawler.crawler - starting 2 parser threads...\n",
      "2025-09-20 01:30:39,239 - INFO - icrawler.crawler - starting 4 downloader threads...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading images for: public trash pile\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-20 01:30:39,538 - INFO - parser - parsing result page https://www.google.com/search?q=public+trash+pile&ijn=0&start=0&tbs=&tbm=isch\n",
      "Exception in thread parser-001:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python311\\Lib\\threading.py\", line 1045, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\SWARNABHA\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"C:\\Program Files\\Python311\\Lib\\threading.py\", line 982, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\SWARNABHA\\AppData\\Roaming\\Python\\Python311\\site-packages\\icrawler\\parser.py\", line 93, in worker_exec\n",
      "    for task in self.parse(response, **kwargs):\n",
      "TypeError: 'NoneType' object is not iterable\n",
      "2025-09-20 01:30:41,243 - INFO - parser - no more page urls for thread parser-002 to parse\n",
      "2025-09-20 01:30:41,247 - INFO - parser - thread parser-002 exit\n",
      "2025-09-20 01:30:44,242 - INFO - downloader - no more download task for thread downloader-001\n",
      "2025-09-20 01:30:44,244 - INFO - downloader - no more download task for thread downloader-002\n",
      "2025-09-20 01:30:44,244 - INFO - downloader - no more download task for thread downloader-003\n",
      "2025-09-20 01:30:44,245 - INFO - downloader - no more download task for thread downloader-004\n",
      "2025-09-20 01:30:44,247 - INFO - downloader - thread downloader-001 exit\n",
      "2025-09-20 01:30:44,249 - INFO - downloader - thread downloader-002 exit\n",
      "2025-09-20 01:30:44,252 - INFO - downloader - thread downloader-003 exit\n",
      "2025-09-20 01:30:44,256 - INFO - downloader - thread downloader-004 exit\n",
      "2025-09-20 01:30:45,249 - INFO - icrawler.crawler - Crawling task done!\n",
      "2025-09-20 01:30:45,251 - INFO - icrawler.crawler - start crawling...\n",
      "2025-09-20 01:30:45,251 - INFO - icrawler.crawler - starting 1 feeder threads...\n",
      "2025-09-20 01:30:45,252 - INFO - feeder - thread feeder-001 exit\n",
      "2025-09-20 01:30:45,253 - INFO - icrawler.crawler - starting 2 parser threads...\n",
      "2025-09-20 01:30:45,255 - INFO - icrawler.crawler - starting 4 downloader threads...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading images for: roadside garbage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-20 01:30:45,557 - INFO - parser - parsing result page https://www.google.com/search?q=roadside+garbage&ijn=0&start=0&tbs=&tbm=isch\n",
      "Exception in thread parser-001:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python311\\Lib\\threading.py\", line 1045, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\SWARNABHA\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"C:\\Program Files\\Python311\\Lib\\threading.py\", line 982, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\SWARNABHA\\AppData\\Roaming\\Python\\Python311\\site-packages\\icrawler\\parser.py\", line 93, in worker_exec\n",
      "    for task in self.parse(response, **kwargs):\n",
      "TypeError: 'NoneType' object is not iterable\n",
      "2025-09-20 01:30:47,259 - INFO - parser - no more page urls for thread parser-002 to parse\n",
      "2025-09-20 01:30:47,260 - INFO - parser - thread parser-002 exit\n",
      "2025-09-20 01:30:50,265 - INFO - downloader - no more download task for thread downloader-004\n",
      "2025-09-20 01:30:50,269 - INFO - downloader - no more download task for thread downloader-002\n",
      "2025-09-20 01:30:50,269 - INFO - downloader - no more download task for thread downloader-003\n",
      "2025-09-20 01:30:50,270 - INFO - downloader - no more download task for thread downloader-001\n",
      "2025-09-20 01:30:50,304 - INFO - downloader - thread downloader-001 exit\n",
      "2025-09-20 01:30:50,298 - INFO - downloader - thread downloader-002 exit\n",
      "2025-09-20 01:30:50,301 - INFO - downloader - thread downloader-003 exit\n",
      "2025-09-20 01:30:50,270 - INFO - downloader - thread downloader-004 exit\n",
      "2025-09-20 01:30:51,259 - INFO - icrawler.crawler - Crawling task done!\n",
      "2025-09-20 01:30:51,259 - INFO - icrawler.crawler - start crawling...\n",
      "2025-09-20 01:30:51,259 - INFO - icrawler.crawler - starting 1 feeder threads...\n",
      "2025-09-20 01:30:51,264 - INFO - feeder - thread feeder-001 exit\n",
      "2025-09-20 01:30:51,268 - INFO - icrawler.crawler - starting 2 parser threads...\n",
      "2025-09-20 01:30:51,277 - INFO - icrawler.crawler - starting 4 downloader threads...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading images for: plastic waste street\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-20 01:30:51,597 - INFO - parser - parsing result page https://www.google.com/search?q=plastic+waste+street&ijn=0&start=0&tbs=&tbm=isch\n",
      "Exception in thread parser-001:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python311\\Lib\\threading.py\", line 1045, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\SWARNABHA\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"C:\\Program Files\\Python311\\Lib\\threading.py\", line 982, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\SWARNABHA\\AppData\\Roaming\\Python\\Python311\\site-packages\\icrawler\\parser.py\", line 93, in worker_exec\n",
      "    for task in self.parse(response, **kwargs):\n",
      "TypeError: 'NoneType' object is not iterable\n",
      "2025-09-20 01:30:53,277 - INFO - parser - no more page urls for thread parser-002 to parse\n",
      "2025-09-20 01:30:53,277 - INFO - parser - thread parser-002 exit\n",
      "2025-09-20 01:30:56,279 - INFO - downloader - no more download task for thread downloader-001\n",
      "2025-09-20 01:30:56,296 - INFO - downloader - thread downloader-001 exit\n",
      "2025-09-20 01:30:56,296 - INFO - downloader - no more download task for thread downloader-002\n",
      "2025-09-20 01:30:56,296 - INFO - downloader - no more download task for thread downloader-004\n",
      "2025-09-20 01:30:56,296 - INFO - downloader - no more download task for thread downloader-003\n",
      "2025-09-20 01:30:56,304 - INFO - downloader - thread downloader-002 exit\n",
      "2025-09-20 01:30:56,307 - INFO - downloader - thread downloader-004 exit\n",
      "2025-09-20 01:30:56,314 - INFO - downloader - thread downloader-003 exit\n",
      "2025-09-20 01:30:57,297 - INFO - icrawler.crawler - Crawling task done!\n",
      "2025-09-20 01:30:57,297 - INFO - icrawler.crawler - start crawling...\n",
      "2025-09-20 01:30:57,300 - INFO - icrawler.crawler - starting 1 feeder threads...\n",
      "2025-09-20 01:30:57,304 - INFO - feeder - thread feeder-001 exit\n",
      "2025-09-20 01:30:57,305 - INFO - icrawler.crawler - starting 2 parser threads...\n",
      "2025-09-20 01:30:57,307 - INFO - icrawler.crawler - starting 4 downloader threads...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading images for: illegal dumping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-20 01:30:57,594 - INFO - parser - parsing result page https://www.google.com/search?q=illegal+dumping&ijn=0&start=0&tbs=&tbm=isch\n",
      "Exception in thread parser-001:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python311\\Lib\\threading.py\", line 1045, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\SWARNABHA\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"C:\\Program Files\\Python311\\Lib\\threading.py\", line 982, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\SWARNABHA\\AppData\\Roaming\\Python\\Python311\\site-packages\\icrawler\\parser.py\", line 93, in worker_exec\n",
      "    for task in self.parse(response, **kwargs):\n",
      "TypeError: 'NoneType' object is not iterable\n",
      "2025-09-20 01:30:59,311 - INFO - parser - no more page urls for thread parser-002 to parse\n",
      "2025-09-20 01:30:59,316 - INFO - parser - thread parser-002 exit\n",
      "2025-09-20 01:31:02,316 - INFO - downloader - no more download task for thread downloader-004\n",
      "2025-09-20 01:31:02,316 - INFO - downloader - no more download task for thread downloader-001\n",
      "2025-09-20 01:31:02,318 - INFO - downloader - no more download task for thread downloader-002\n",
      "2025-09-20 01:31:02,318 - INFO - downloader - no more download task for thread downloader-003\n",
      "2025-09-20 01:31:02,318 - INFO - downloader - thread downloader-004 exit\n",
      "2025-09-20 01:31:02,320 - INFO - downloader - thread downloader-001 exit\n",
      "2025-09-20 01:31:02,321 - INFO - downloader - thread downloader-002 exit\n",
      "2025-09-20 01:31:02,335 - INFO - downloader - thread downloader-003 exit\n",
      "2025-09-20 01:31:03,319 - INFO - icrawler.crawler - Crawling task done!\n"
     ]
    }
   ],
   "source": [
    "from icrawler.builtin import GoogleImageCrawler\n",
    "from icrawler import ImageDownloader\n",
    "\n",
    "# Custom downloader to avoid 403 errors\n",
    "class MyDownloader(ImageDownloader):\n",
    "    def download(self, task, default_ext, timeout=5, **kwargs):\n",
    "        self.session.headers.update({'User-Agent': 'Mozilla/5.0'})\n",
    "        return super().download(task, default_ext, timeout=timeout, **kwargs)\n",
    "\n",
    "# Create crawler\n",
    "crawler = GoogleImageCrawler(\n",
    "    storage={'root_dir': r\"C:/garbage_img\"},\n",
    "    feeder_threads=1,\n",
    "    parser_threads=2,\n",
    "    downloader_threads=4,\n",
    "    downloader_cls=MyDownloader\n",
    ")\n",
    "\n",
    "garbage_keywords = [\n",
    "    \"street garbage\",\n",
    "    \"garbage dump\",\n",
    "    \"overflowing trash bin\",\n",
    "    \"litter on road\",\n",
    "    \"urban waste\",\n",
    "    \"waste disposal site\",\n",
    "    \"public trash pile\",\n",
    "    \"roadside garbage\",\n",
    "    \"plastic waste street\",\n",
    "    \"illegal dumping\"\n",
    "]\n",
    "\n",
    "# Crawl images for each keyword\n",
    "images_per_keyword = 100  # 10 keywords × 100 = 1000 attempts\n",
    "for i, keyword in enumerate(garbage_keywords):\n",
    "    print(f\"Downloading images for: {keyword}\")\n",
    "    crawler.crawl(\n",
    "        keyword=keyword,\n",
    "        max_num=images_per_keyword,\n",
    "        file_idx_offset=i * images_per_keyword\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cba8ae75-d1f8-45d7-8365-755c087500f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting roboflow\n",
      "  Using cached roboflow-1.2.9-py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: certifi in c:\\users\\swarnabha\\appdata\\roaming\\python\\python311\\site-packages (from roboflow) (2025.1.31)\n",
      "Collecting idna==3.7 (from roboflow)\n",
      "  Using cached idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: cycler in c:\\users\\swarnabha\\appdata\\roaming\\python\\python311\\site-packages (from roboflow) (0.12.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\swarnabha\\appdata\\roaming\\python\\python311\\site-packages (from roboflow) (1.4.8)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\swarnabha\\appdata\\roaming\\python\\python311\\site-packages (from roboflow) (3.10.1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\swarnabha\\appdata\\roaming\\python\\python311\\site-packages (from roboflow) (2.2.3)\n",
      "Collecting opencv-python-headless==4.10.0.84 (from roboflow)\n",
      "  Using cached opencv_python_headless-4.10.0.84-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: Pillow>=7.1.2 in c:\\users\\swarnabha\\appdata\\roaming\\python\\python311\\site-packages (from roboflow) (11.1.0)\n",
      "Requirement already satisfied: pi-heif<2 in c:\\users\\swarnabha\\appdata\\roaming\\python\\python311\\site-packages (from roboflow) (1.1.0)\n",
      "Requirement already satisfied: pillow-avif-plugin<2 in c:\\users\\swarnabha\\appdata\\roaming\\python\\python311\\site-packages (from roboflow) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\swarnabha\\appdata\\roaming\\python\\python311\\site-packages (from roboflow) (2.9.0.post0)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\swarnabha\\appdata\\roaming\\python\\python311\\site-packages (from roboflow) (1.1.1)\n",
      "Requirement already satisfied: requests in c:\\users\\swarnabha\\appdata\\roaming\\python\\python311\\site-packages (from roboflow) (2.32.3)\n",
      "Requirement already satisfied: six in c:\\users\\swarnabha\\appdata\\roaming\\python\\python311\\site-packages (from roboflow) (1.17.0)\n",
      "Requirement already satisfied: urllib3>=1.26.6 in c:\\users\\swarnabha\\appdata\\roaming\\python\\python311\\site-packages (from roboflow) (2.3.0)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in c:\\users\\swarnabha\\appdata\\roaming\\python\\python311\\site-packages (from roboflow) (4.67.1)\n",
      "Requirement already satisfied: PyYAML>=5.3.1 in c:\\users\\swarnabha\\appdata\\roaming\\python\\python311\\site-packages (from roboflow) (6.0.2)\n",
      "Collecting requests-toolbelt (from roboflow)\n",
      "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: filetype in c:\\users\\swarnabha\\appdata\\roaming\\python\\python311\\site-packages (from roboflow) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\swarnabha\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.41.0->roboflow) (0.4.6)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\swarnabha\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->roboflow) (1.3.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\swarnabha\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->roboflow) (4.56.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\swarnabha\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->roboflow) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\swarnabha\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->roboflow) (3.2.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\swarnabha\\appdata\\roaming\\python\\python311\\site-packages (from requests->roboflow) (3.4.1)\n",
      "Using cached roboflow-1.2.9-py3-none-any.whl (88 kB)\n",
      "Using cached idna-3.7-py3-none-any.whl (66 kB)\n",
      "Using cached opencv_python_headless-4.10.0.84-cp37-abi3-win_amd64.whl (38.8 MB)\n",
      "Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Installing collected packages: opencv-python-headless, idna, requests-toolbelt, roboflow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\SWARNABHA\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages\\\\cv2\\\\cv2.pyd'\n",
      "Check the permissions.\n",
      "\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install roboflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3aa7b569-23a4-4772-a915-3189d0cebf15",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'roboflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mroboflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Roboflow\n\u001b[32m      3\u001b[39m rf = Roboflow(api_key=\u001b[33m\"\u001b[39m\u001b[33m2Qa6sAu7a0C1lG7KvPrq\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Replace with your workspace and project names\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'roboflow'"
     ]
    }
   ],
   "source": [
    "from roboflow import Roboflow\n",
    "\n",
    "rf = Roboflow(api_key=\"2Qa6sAu7a0C1lG7KvPrq\")\n",
    "\n",
    "# Replace with your workspace and project names\n",
    "project = rf.workspace(\"Swarnabha\").project(\"garbage_detection\")\n",
    "\n",
    "# Specify the version number of the dataset (e.g., 1)\n",
    "dataset = project.version(1).download(\"images\")  # downloads raw images only\n",
    "\n",
    "print(\"Download completed! Images saved at:\", dataset.location)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dcaa3030-dbce-4a63-87f3-0d600bf815ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting roboflow\n",
      "  Using cached roboflow-1.2.9-py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: certifi in c:\\users\\swarnabha\\appdata\\roaming\\python\\python311\\site-packages (from roboflow) (2025.1.31)\n",
      "Collecting idna==3.7 (from roboflow)\n",
      "  Using cached idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: cycler in c:\\users\\swarnabha\\appdata\\roaming\\python\\python311\\site-packages (from roboflow) (0.12.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\swarnabha\\appdata\\roaming\\python\\python311\\site-packages (from roboflow) (1.4.8)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\swarnabha\\appdata\\roaming\\python\\python311\\site-packages (from roboflow) (3.10.1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\swarnabha\\appdata\\roaming\\python\\python311\\site-packages (from roboflow) (2.2.3)\n",
      "Collecting opencv-python-headless==4.10.0.84 (from roboflow)\n",
      "  Using cached opencv_python_headless-4.10.0.84-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: Pillow>=7.1.2 in c:\\users\\swarnabha\\appdata\\roaming\\python\\python311\\site-packages (from roboflow) (11.1.0)\n",
      "Requirement already satisfied: pi-heif<2 in c:\\users\\swarnabha\\appdata\\roaming\\python\\python311\\site-packages (from roboflow) (1.1.0)\n",
      "Requirement already satisfied: pillow-avif-plugin<2 in c:\\users\\swarnabha\\appdata\\roaming\\python\\python311\\site-packages (from roboflow) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\swarnabha\\appdata\\roaming\\python\\python311\\site-packages (from roboflow) (2.9.0.post0)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\swarnabha\\appdata\\roaming\\python\\python311\\site-packages (from roboflow) (1.1.1)\n",
      "Requirement already satisfied: requests in c:\\users\\swarnabha\\appdata\\roaming\\python\\python311\\site-packages (from roboflow) (2.32.3)\n",
      "Requirement already satisfied: six in c:\\users\\swarnabha\\appdata\\roaming\\python\\python311\\site-packages (from roboflow) (1.17.0)\n",
      "Requirement already satisfied: urllib3>=1.26.6 in c:\\users\\swarnabha\\appdata\\roaming\\python\\python311\\site-packages (from roboflow) (2.3.0)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in c:\\users\\swarnabha\\appdata\\roaming\\python\\python311\\site-packages (from roboflow) (4.67.1)\n",
      "Requirement already satisfied: PyYAML>=5.3.1 in c:\\users\\swarnabha\\appdata\\roaming\\python\\python311\\site-packages (from roboflow) (6.0.2)\n",
      "Collecting requests-toolbelt (from roboflow)\n",
      "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: filetype in c:\\users\\swarnabha\\appdata\\roaming\\python\\python311\\site-packages (from roboflow) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\swarnabha\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.41.0->roboflow) (0.4.6)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\swarnabha\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->roboflow) (1.3.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\swarnabha\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->roboflow) (4.56.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\swarnabha\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->roboflow) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\swarnabha\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->roboflow) (3.2.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\swarnabha\\appdata\\roaming\\python\\python311\\site-packages (from requests->roboflow) (3.4.1)\n",
      "Using cached roboflow-1.2.9-py3-none-any.whl (88 kB)\n",
      "Using cached idna-3.7-py3-none-any.whl (66 kB)\n",
      "Using cached opencv_python_headless-4.10.0.84-cp37-abi3-win_amd64.whl (38.8 MB)\n",
      "Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Installing collected packages: opencv-python-headless, idna, requests-toolbelt, roboflow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\SWARNABHA\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages\\\\cv2\\\\cv2.pyd'\n",
      "Check the permissions.\n",
      "\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install roboflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c5a8cf-2b6e-42ca-8f37-520d1e410de9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
